{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMq5GJAcF0GASLFlCMHZh5R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaya-02ly/akshaya2260-nlp/blob/main/lab_8_2(nlp).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CORPUS"
      ],
      "metadata": {
        "id": "1sppWC0kckc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text='''\n",
        "D1:\n",
        "Artificial intelligence is transforming global industries at an unprecedented velocity.\n",
        "\n",
        "D2:\n",
        "Recent developments in quantum cryptography and blockchain ledger technologies are redefining cybersecurity frameworks.\n",
        "\n",
        "D3:\n",
        "The metaverse ecosystem integrates augmented reality, neural networks, and decentralized finance protocols.\n",
        "\n",
        "D4:\n",
        "Researchers at international laboratories are studying neuroplasticity and bioinformatics to enhance predictive analytics.\n",
        "\n",
        "D5:\n",
        "Autonomous vehicles utilize advanced sensor fusion mechanisms to navigate complex urban landscapes.\n",
        "\n",
        "D6:\n",
        "Deep learning systems for natural language processing are achieving human-level comprehension in various linguistic tasks.\n",
        "\n",
        "D7:\n",
        "The adoption of edge computing paradigms is enabling real-time data processing closer to the source.\n",
        "\n",
        "D8:\n",
        "Edge computing reduces latency in distributed systems.\n",
        "\n",
        "D9:\n",
        "Edge computing improves efficiency in IoT deployments.\n",
        "\n",
        "D10:\n",
        "Advanced AI ecosystems are integrating multiple emerging technologies to drive digital transformation.'''"
      ],
      "metadata": {
        "id": "F9TVWigHbP47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uni Gram Counts"
      ],
      "metadata": {
        "id": "Cm3iPPJ6camr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "# Use the 'text' variable from the previously loaded corpus\n",
        "words = text.lower().split()\n",
        "\n",
        "# Calculate unigram counts\n",
        "unigram_counts = collections.Counter(words)\n",
        "\n",
        "# Print the unigram counts\n",
        "print(\"Unigram Counts:\")\n",
        "for word, count in unigram_counts.most_common():\n",
        "    print(f\"{word}: {count}\")\n",
        "#Vocabulary size is length of unigrams\n",
        "V=len(unigram_counts)\n",
        "print(\"Vocabulary Size=\",V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-chdArSZaMF",
        "outputId": "781b2cba-bf11-4d3a-bcb1-8385ab3e8a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Counts:\n",
            "in: 4\n",
            "are: 4\n",
            "to: 4\n",
            "and: 3\n",
            "the: 3\n",
            "edge: 3\n",
            "computing: 3\n",
            "is: 2\n",
            "at: 2\n",
            "technologies: 2\n",
            "advanced: 2\n",
            "processing: 2\n",
            "d1:: 1\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "transforming: 1\n",
            "global: 1\n",
            "industries: 1\n",
            "an: 1\n",
            "unprecedented: 1\n",
            "velocity.: 1\n",
            "d2:: 1\n",
            "recent: 1\n",
            "developments: 1\n",
            "quantum: 1\n",
            "cryptography: 1\n",
            "blockchain: 1\n",
            "ledger: 1\n",
            "redefining: 1\n",
            "cybersecurity: 1\n",
            "frameworks.: 1\n",
            "d3:: 1\n",
            "metaverse: 1\n",
            "ecosystem: 1\n",
            "integrates: 1\n",
            "augmented: 1\n",
            "reality,: 1\n",
            "neural: 1\n",
            "networks,: 1\n",
            "decentralized: 1\n",
            "finance: 1\n",
            "protocols.: 1\n",
            "d4:: 1\n",
            "researchers: 1\n",
            "international: 1\n",
            "laboratories: 1\n",
            "studying: 1\n",
            "neuroplasticity: 1\n",
            "bioinformatics: 1\n",
            "enhance: 1\n",
            "predictive: 1\n",
            "analytics.: 1\n",
            "d5:: 1\n",
            "autonomous: 1\n",
            "vehicles: 1\n",
            "utilize: 1\n",
            "sensor: 1\n",
            "fusion: 1\n",
            "mechanisms: 1\n",
            "navigate: 1\n",
            "complex: 1\n",
            "urban: 1\n",
            "landscapes.: 1\n",
            "d6:: 1\n",
            "deep: 1\n",
            "learning: 1\n",
            "systems: 1\n",
            "for: 1\n",
            "natural: 1\n",
            "language: 1\n",
            "achieving: 1\n",
            "human-level: 1\n",
            "comprehension: 1\n",
            "various: 1\n",
            "linguistic: 1\n",
            "tasks.: 1\n",
            "d7:: 1\n",
            "adoption: 1\n",
            "of: 1\n",
            "paradigms: 1\n",
            "enabling: 1\n",
            "real-time: 1\n",
            "data: 1\n",
            "closer: 1\n",
            "source.: 1\n",
            "d8:: 1\n",
            "reduces: 1\n",
            "latency: 1\n",
            "distributed: 1\n",
            "systems.: 1\n",
            "d9:: 1\n",
            "improves: 1\n",
            "efficiency: 1\n",
            "iot: 1\n",
            "deployments.: 1\n",
            "d10:: 1\n",
            "ai: 1\n",
            "ecosystems: 1\n",
            "integrating: 1\n",
            "multiple: 1\n",
            "emerging: 1\n",
            "drive: 1\n",
            "digital: 1\n",
            "transformation.: 1\n",
            "Vocabulary Size= 104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bi-gram count"
      ],
      "metadata": {
        "id": "RQtrQoQBcFwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "# Use the 'text' variable from the previously loaded corpus\n",
        "words = text.lower().split()\n",
        "\n",
        "# Generate bigrams\n",
        "bigrams = []\n",
        "for i in range(len(words) - 1):\n",
        "    bigrams.append((words[i], words[i+1]))\n",
        "\n",
        "# Calculate bigram counts\n",
        "bigram_counts = collections.Counter(bigrams)\n",
        "\n",
        "# Print the bigram counts\n",
        "print(\"\\nBigram Counts:\")\n",
        "for bigram, count in bigram_counts.most_common():\n",
        "    print(f\"{bigram[0]} {bigram[1]}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgvPd-z-Zuf0",
        "outputId": "a8f5079c-0ecd-4e47-d4b6-44878d91611c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigram Counts:\n",
            "edge computing: 3\n",
            "d1: artificial: 1\n",
            "artificial intelligence: 1\n",
            "intelligence is: 1\n",
            "is transforming: 1\n",
            "transforming global: 1\n",
            "global industries: 1\n",
            "industries at: 1\n",
            "at an: 1\n",
            "an unprecedented: 1\n",
            "unprecedented velocity.: 1\n",
            "velocity. d2:: 1\n",
            "d2: recent: 1\n",
            "recent developments: 1\n",
            "developments in: 1\n",
            "in quantum: 1\n",
            "quantum cryptography: 1\n",
            "cryptography and: 1\n",
            "and blockchain: 1\n",
            "blockchain ledger: 1\n",
            "ledger technologies: 1\n",
            "technologies are: 1\n",
            "are redefining: 1\n",
            "redefining cybersecurity: 1\n",
            "cybersecurity frameworks.: 1\n",
            "frameworks. d3:: 1\n",
            "d3: the: 1\n",
            "the metaverse: 1\n",
            "metaverse ecosystem: 1\n",
            "ecosystem integrates: 1\n",
            "integrates augmented: 1\n",
            "augmented reality,: 1\n",
            "reality, neural: 1\n",
            "neural networks,: 1\n",
            "networks, and: 1\n",
            "and decentralized: 1\n",
            "decentralized finance: 1\n",
            "finance protocols.: 1\n",
            "protocols. d4:: 1\n",
            "d4: researchers: 1\n",
            "researchers at: 1\n",
            "at international: 1\n",
            "international laboratories: 1\n",
            "laboratories are: 1\n",
            "are studying: 1\n",
            "studying neuroplasticity: 1\n",
            "neuroplasticity and: 1\n",
            "and bioinformatics: 1\n",
            "bioinformatics to: 1\n",
            "to enhance: 1\n",
            "enhance predictive: 1\n",
            "predictive analytics.: 1\n",
            "analytics. d5:: 1\n",
            "d5: autonomous: 1\n",
            "autonomous vehicles: 1\n",
            "vehicles utilize: 1\n",
            "utilize advanced: 1\n",
            "advanced sensor: 1\n",
            "sensor fusion: 1\n",
            "fusion mechanisms: 1\n",
            "mechanisms to: 1\n",
            "to navigate: 1\n",
            "navigate complex: 1\n",
            "complex urban: 1\n",
            "urban landscapes.: 1\n",
            "landscapes. d6:: 1\n",
            "d6: deep: 1\n",
            "deep learning: 1\n",
            "learning systems: 1\n",
            "systems for: 1\n",
            "for natural: 1\n",
            "natural language: 1\n",
            "language processing: 1\n",
            "processing are: 1\n",
            "are achieving: 1\n",
            "achieving human-level: 1\n",
            "human-level comprehension: 1\n",
            "comprehension in: 1\n",
            "in various: 1\n",
            "various linguistic: 1\n",
            "linguistic tasks.: 1\n",
            "tasks. d7:: 1\n",
            "d7: the: 1\n",
            "the adoption: 1\n",
            "adoption of: 1\n",
            "of edge: 1\n",
            "computing paradigms: 1\n",
            "paradigms is: 1\n",
            "is enabling: 1\n",
            "enabling real-time: 1\n",
            "real-time data: 1\n",
            "data processing: 1\n",
            "processing closer: 1\n",
            "closer to: 1\n",
            "to the: 1\n",
            "the source.: 1\n",
            "source. d8:: 1\n",
            "d8: edge: 1\n",
            "computing reduces: 1\n",
            "reduces latency: 1\n",
            "latency in: 1\n",
            "in distributed: 1\n",
            "distributed systems.: 1\n",
            "systems. d9:: 1\n",
            "d9: edge: 1\n",
            "computing improves: 1\n",
            "improves efficiency: 1\n",
            "efficiency in: 1\n",
            "in iot: 1\n",
            "iot deployments.: 1\n",
            "deployments. d10:: 1\n",
            "d10: advanced: 1\n",
            "advanced ai: 1\n",
            "ai ecosystems: 1\n",
            "ecosystems are: 1\n",
            "are integrating: 1\n",
            "integrating multiple: 1\n",
            "multiple emerging: 1\n",
            "emerging technologies: 1\n",
            "technologies to: 1\n",
            "to drive: 1\n",
            "drive digital: 1\n",
            "digital transformation.: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tri-gram count"
      ],
      "metadata": {
        "id": "_8xHuAKOcNBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "# Use the 'text' variable from the previously loaded corpus\n",
        "words = text.lower().split()\n",
        "\n",
        "# Generate trigrams\n",
        "Trigrams = []\n",
        "for i in range(len(words) - 2):\n",
        "    Trigrams.append((words[i], words[i+1], words[i+2]))\n",
        "\n",
        "# Calculate trigram counts\n",
        "Trigrams_counts = collections.Counter(Trigrams)\n",
        "\n",
        "# Print the trigram counts\n",
        "print(\"\\nTrigrams Counts:\")\n",
        "for Trigrams, count in Trigrams_counts.most_common():\n",
        "    print(f\"{Trigrams[0]} {Trigrams[1]} {Trigrams[2]}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkHy2myXaMKD",
        "outputId": "0c9e0e12-95fc-4e29-87bb-c626799918b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trigrams Counts:\n",
            "d1: artificial intelligence: 1\n",
            "artificial intelligence is: 1\n",
            "intelligence is transforming: 1\n",
            "is transforming global: 1\n",
            "transforming global industries: 1\n",
            "global industries at: 1\n",
            "industries at an: 1\n",
            "at an unprecedented: 1\n",
            "an unprecedented velocity.: 1\n",
            "unprecedented velocity. d2:: 1\n",
            "velocity. d2: recent: 1\n",
            "d2: recent developments: 1\n",
            "recent developments in: 1\n",
            "developments in quantum: 1\n",
            "in quantum cryptography: 1\n",
            "quantum cryptography and: 1\n",
            "cryptography and blockchain: 1\n",
            "and blockchain ledger: 1\n",
            "blockchain ledger technologies: 1\n",
            "ledger technologies are: 1\n",
            "technologies are redefining: 1\n",
            "are redefining cybersecurity: 1\n",
            "redefining cybersecurity frameworks.: 1\n",
            "cybersecurity frameworks. d3:: 1\n",
            "frameworks. d3: the: 1\n",
            "d3: the metaverse: 1\n",
            "the metaverse ecosystem: 1\n",
            "metaverse ecosystem integrates: 1\n",
            "ecosystem integrates augmented: 1\n",
            "integrates augmented reality,: 1\n",
            "augmented reality, neural: 1\n",
            "reality, neural networks,: 1\n",
            "neural networks, and: 1\n",
            "networks, and decentralized: 1\n",
            "and decentralized finance: 1\n",
            "decentralized finance protocols.: 1\n",
            "finance protocols. d4:: 1\n",
            "protocols. d4: researchers: 1\n",
            "d4: researchers at: 1\n",
            "researchers at international: 1\n",
            "at international laboratories: 1\n",
            "international laboratories are: 1\n",
            "laboratories are studying: 1\n",
            "are studying neuroplasticity: 1\n",
            "studying neuroplasticity and: 1\n",
            "neuroplasticity and bioinformatics: 1\n",
            "and bioinformatics to: 1\n",
            "bioinformatics to enhance: 1\n",
            "to enhance predictive: 1\n",
            "enhance predictive analytics.: 1\n",
            "predictive analytics. d5:: 1\n",
            "analytics. d5: autonomous: 1\n",
            "d5: autonomous vehicles: 1\n",
            "autonomous vehicles utilize: 1\n",
            "vehicles utilize advanced: 1\n",
            "utilize advanced sensor: 1\n",
            "advanced sensor fusion: 1\n",
            "sensor fusion mechanisms: 1\n",
            "fusion mechanisms to: 1\n",
            "mechanisms to navigate: 1\n",
            "to navigate complex: 1\n",
            "navigate complex urban: 1\n",
            "complex urban landscapes.: 1\n",
            "urban landscapes. d6:: 1\n",
            "landscapes. d6: deep: 1\n",
            "d6: deep learning: 1\n",
            "deep learning systems: 1\n",
            "learning systems for: 1\n",
            "systems for natural: 1\n",
            "for natural language: 1\n",
            "natural language processing: 1\n",
            "language processing are: 1\n",
            "processing are achieving: 1\n",
            "are achieving human-level: 1\n",
            "achieving human-level comprehension: 1\n",
            "human-level comprehension in: 1\n",
            "comprehension in various: 1\n",
            "in various linguistic: 1\n",
            "various linguistic tasks.: 1\n",
            "linguistic tasks. d7:: 1\n",
            "tasks. d7: the: 1\n",
            "d7: the adoption: 1\n",
            "the adoption of: 1\n",
            "adoption of edge: 1\n",
            "of edge computing: 1\n",
            "edge computing paradigms: 1\n",
            "computing paradigms is: 1\n",
            "paradigms is enabling: 1\n",
            "is enabling real-time: 1\n",
            "enabling real-time data: 1\n",
            "real-time data processing: 1\n",
            "data processing closer: 1\n",
            "processing closer to: 1\n",
            "closer to the: 1\n",
            "to the source.: 1\n",
            "the source. d8:: 1\n",
            "source. d8: edge: 1\n",
            "d8: edge computing: 1\n",
            "edge computing reduces: 1\n",
            "computing reduces latency: 1\n",
            "reduces latency in: 1\n",
            "latency in distributed: 1\n",
            "in distributed systems.: 1\n",
            "distributed systems. d9:: 1\n",
            "systems. d9: edge: 1\n",
            "d9: edge computing: 1\n",
            "edge computing improves: 1\n",
            "computing improves efficiency: 1\n",
            "improves efficiency in: 1\n",
            "efficiency in iot: 1\n",
            "in iot deployments.: 1\n",
            "iot deployments. d10:: 1\n",
            "deployments. d10: advanced: 1\n",
            "d10: advanced ai: 1\n",
            "advanced ai ecosystems: 1\n",
            "ai ecosystems are: 1\n",
            "ecosystems are integrating: 1\n",
            "are integrating multiple: 1\n",
            "integrating multiple emerging: 1\n",
            "multiple emerging technologies: 1\n",
            "emerging technologies to: 1\n",
            "technologies to drive: 1\n",
            "to drive digital: 1\n",
            "drive digital transformation.: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Bi-Gram Counts"
      ],
      "metadata": {
        "id": "lqmX8ip8cuZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram(word_sequence, bigram_counts, unigram_counts):\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = bigram_count / last_word_unigram_count\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"artificial intelligence\"\n",
        "next_word1 = predict_next_word_bigram(sequence1, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"edge computing\"\n",
        "next_word2 = predict_next_word_bigram(sequence2, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"deep learning\"\n",
        "next_word3 = predict_next_word_bigram(sequence3, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram(sequence4, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hsw2ercai9M",
        "outputId": "81106719-efee-43ae-e8be-70b171215ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  is is  1.0\n",
            "Given sequence: 'artificial intelligence', predicted next word: 'is'\n",
            "probability of  paradigms is  0.3333333333333333\n",
            "probability of  reduces is  0.3333333333333333\n",
            "probability of  improves is  0.3333333333333333\n",
            "Given sequence: 'edge computing', predicted next word: 'paradigms'\n",
            "probability of  systems is  1.0\n",
            "Given sequence: 'deep learning', predicted next word: 'systems'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "eployment of Bi-Gram Model"
      ],
      "metadata": {
        "id": "zcTVLJUvdab1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram(ip_text, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "id": "ZUFNaxfNcsFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Tri-Gram Counts"
      ],
      "metadata": {
        "id": "lcDwyYkgdtCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram(word_sequence, Trigrams_counts, bigram_counts):\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = trigram_count / last_two_words_bigram_count\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram(sequence1, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram(sequence2, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "id": "tun202a5c-nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment of Tri-Gram Model"
      ],
      "metadata": {
        "id": "LfNZONmkd3WM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram(ip_text, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "id": "ihahuzcyhe7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Bi-Gram Counts with Laplace Smoothening"
      ],
      "metadata": {
        "id": "h8h8dRd5d_o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram_Laplace(word_sequence, bigram_counts, unigram_counts):\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = (bigram_count+1) / (last_word_unigram_count+V)\n",
        "        print(\"probability of \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram_Laplace(sequence1, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_bigram_Laplace(sequence2, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"professor I\"\n",
        "next_word3 = predict_next_word_bigram_Laplace(sequence3, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram_Laplace(sequence4, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "id": "8UEQ-55Hfkho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment of Laplace Smoothening based Bi-Gram Model"
      ],
      "metadata": {
        "id": "CDWYRqut2_3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram_Laplace(ip_text, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "id": "c4BQbeCzhHm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Tri-Gram Counts based on laplace smoothening"
      ],
      "metadata": {
        "id": "1bJJ_EPz3ai2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram_Laplace(word_sequence, Trigrams_counts, bigram_counts):\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = (trigram_count+1) / (last_two_words_bigram_count+V)\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram_Laplace(sequence1, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram_Laplace(sequence2, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz-Fzd02lYAz",
        "outputId": "6ac7c737-e5c9-4e66-b650-4e7ed1cf90e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  as is  0.11764705882352941\n",
            "probability of  in is  0.11764705882352941\n",
            "Given sequence: 'I am working', predicted next word: 'as'\n",
            "probability of  phd is  0.11764705882352941\n",
            "probability of  masters is  0.11764705882352941\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment of Laplace Smoothening based Tri-Gram Model"
      ],
      "metadata": {
        "id": "so-pzbZc3nGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram_Laplace(ip_text, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGnCcIN3lrD0",
        "outputId": "efe4ed7f-c05b-4ac4-ccdd-10005746f98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am working \n",
            "probability of  as is  0.11764705882352941\n",
            "probability of  in is  0.11764705882352941\n",
            "Given sequence: 'I am working ', predicted next word: 'as'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Bi-Gram Counts with Add - K Smoothening"
      ],
      "metadata": {
        "id": "mztPHUmq39_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram_K(word_sequence, bigram_counts, unigram_counts, K): #K=0.5-0.01\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = (bigram_count+K) / (last_word_unigram_count+K*V)\n",
        "        print(\"probability of \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram_K(sequence1, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_bigram_K(sequence2, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"professor I\"\n",
        "next_word3 = predict_next_word_bigram_K(sequence3, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram_K(sequence4, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJoBuea9mbFb",
        "outputId": "6006468d-03e1-4824-ae52-352d2de63d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of working is  0.2631578947368421\n",
            "Given sequence: 'I am', predicted next word: 'working'\n",
            "probability of phd is  0.15789473684210525\n",
            "probability of masters is  0.15789473684210525\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n",
            "probability of am is  0.21739130434782608\n",
            "probability of did is  0.21739130434782608\n",
            "Given sequence: 'professor I', predicted next word: 'am'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment of Add-K Smoothening based Bi-Gram Model"
      ],
      "metadata": {
        "id": "u6ADdhgE4Osc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram_K(ip_text, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOHJhm_QoZx6",
        "outputId": "433256c5-4242-4a89-b67a-f70877dc0aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am\n",
            "probability of working is  0.2631578947368421\n",
            "Given sequence: 'I am', predicted next word: 'working'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Tri-Gram Counts with Add - K Smoothening"
      ],
      "metadata": {
        "id": "ccLnazGU4XwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram_K(word_sequence, Trigrams_counts, bigram_counts,K): #K=0.5-0.01\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = (trigram_count+K) / (last_two_words_bigram_count+K*V)\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram_K(sequence1, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram_K(sequence2, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "id": "iAAZjNF8nTOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment of Add-K Smoothening based Tri-Gram Model"
      ],
      "metadata": {
        "id": "y0MpUhGw4gr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram_K(ip_text, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "id": "ZsmIYasxoNtS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}