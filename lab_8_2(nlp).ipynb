{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFjCXruQ8dhSQnF+Ct3KY/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaya-02ly/akshaya2260-nlp/blob/main/lab_8_2(nlp).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b73d4f2"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires importing several libraries for text processing and related tasks. I will import `nltk` (with `word_tokenize` and `FreqDist`), `collections` (for `defaultdict`), `string`, `math`, and `os`. I will also include `nltk.download('punkt')` to ensure the tokenizer is available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be2550a0",
        "outputId": "9bf99ca3-ab18-4f88-e9c5-80fb0571e067"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import collections\n",
        "import string\n",
        "import math\n",
        "import os\n",
        "\n",
        "# Download the 'punkt' tokenizer if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print(\"Libraries imported successfully and punkt tokenizer checked/downloaded.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully and punkt tokenizer checked/downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c98da870"
      },
      "source": [
        "## Load and Explain Dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the provided text corpus, ensure it's clean, and display a sample. Provide a 5-6 line explanation of the dataset's content and characteristics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d5b7f27"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to create a multiline string variable named `text` with the provided text corpus and then display the first 200 characters to show a sample. This can be achieved in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee4558cb",
        "outputId": "32662d38-3f8b-48a3-f532-7d97b9686e07"
      },
      "source": [
        "text = '''\n",
        "artificial intelligence is transforming global industries at an unprecedented velocity\n",
        "recent developments in quantum cryptography and blockchain ledger technologies\n",
        "are redefining cybersecurity frameworks the metaverse ecosystem integrates\n",
        "augmented reality neural networks and decentralized finance protocols\n",
        "\n",
        "researchers at international laboratories are studying neuroplasticity and\n",
        "bioinformatics to enhance predictive analytics autonomous vehicles utilize\n",
        "advanced sensor fusion mechanisms to navigate complex urban landscapes\n",
        "while deep learning systems for natural language processing are achieving\n",
        "human-level comprehension in various linguistic tasks the adoption of\n",
        "edge computing paradigms is enabling real-time data processing closer\n",
        "to the source reducing latency and improving efficiency in IoT deployments\n",
        "'''\n",
        "\n",
        "print(\"First 200 characters of the text corpus:\\n\")\n",
        "print(text[:200])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 200 characters of the text corpus:\n",
            "\n",
            "\n",
            "artificial intelligence is transforming global industries at an unprecedented velocity\n",
            "recent developments in quantum cryptography and blockchain ledger technologies \n",
            "are redefining cybersecurity fra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8b676ed"
      },
      "source": [
        "```markdown\n",
        "**Dataset Explanation:**\n",
        "\n",
        "This dataset consists of a technical text corpus focusing on emerging technologies. It covers topics such as artificial intelligence, quantum cryptography, blockchain, metaverse, neural networks, bioinformatics, autonomous vehicles, natural language processing, and edge computing. The text describes advancements and applications within these fields, highlighting their impact on industries and various technological paradigms. It appears to be a concise overview of several cutting-edge concepts.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a00f60c"
      },
      "source": [
        "**Dataset Explanation:**\n",
        "\n",
        "This dataset consists of a technical text corpus focusing on emerging technologies. It covers topics such as artificial intelligence, quantum cryptography, blockchain, metaverse, neural networks, bioinformatics, autonomous vehicles, natural language processing, and edge computing. The text describes advancements and applications within these fields, highlighting their impact on industries and various technological paradigms. It appears to be a concise overview of several cutting-edge concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cf6ee24"
      },
      "source": [
        "## Initial Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Perform initial text preprocessing steps: convert the text to lowercase, remove punctuation, and tokenize the text into a list of words. This step prepares the text for N-gram analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4033a8a1"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires converting the text to lowercase, removing punctuation, and tokenizing it. I will use the available `text` variable, `lower()` method, `string.punctuation` for removal, and `nltk.tokenize.word_tokenize` for tokenization, then print the first 20 tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "901629b3",
        "outputId": "abd7b577-10f7-4fb5-e7f4-c7ce91bac3cf"
      },
      "source": [
        "import string\n",
        "\n",
        "# 1. Convert text to lowercase\n",
        "cleaned_text = text.lower()\n",
        "\n",
        "# 2. Remove punctuation\n",
        "no_punct_text = ''.join([char for char in cleaned_text if char not in string.punctuation])\n",
        "\n",
        "# 3. Tokenize the text\n",
        "tokens = word_tokenize(no_punct_text)\n",
        "\n",
        "# 4. Print the first 20 tokens\n",
        "print(\"First 20 tokens after preprocessing:\")\n",
        "print(tokens[:20])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 tokens after preprocessing:\n",
            "['artificial', 'intelligence', 'is', 'transforming', 'global', 'industries', 'at', 'an', 'unprecedented', 'velocity', 'recent', 'developments', 'in', 'quantum', 'cryptography', 'and', 'blockchain', 'ledger', 'technologies', 'are']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e81eeff8"
      },
      "source": [
        "## Identify and Replace Rare Words\n",
        "\n",
        "### Subtask:\n",
        "Identify words that appear only once in the tokenized text. Replace these rare words with a special 'UNK' (unknown) token. Explain why rare words can negatively impact perplexity calculations in language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74a41096"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will calculate the frequency distribution of the tokens to identify words that appear only once, then replace those rare words with 'UNK' in a new list, and finally print the first 20 tokens from the updated list to verify the replacement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4869f153",
        "outputId": "4bb38ae8-46e4-48f0-d439-fbca7cd20589"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "# 1. Calculate the frequency distribution of all tokens\n",
        "fdist = FreqDist(tokens)\n",
        "\n",
        "# 2. Identify all words that appear only once\n",
        "rare_words = set(word for word, freq in fdist.items() if freq == 1)\n",
        "\n",
        "# 3. Create a new list of tokens, replacing rare words with 'UNK'\n",
        "tokens_unk = [word if word not in rare_words else 'UNK' for word in tokens]\n",
        "\n",
        "# 4. Print the first 20 tokens from the tokens_unk list\n",
        "print(\"First 20 tokens after UNK replacement:\")\n",
        "print(tokens_unk[:20])\n",
        "\n",
        "print(f\"Total unique words: {len(fdist)}\")\n",
        "print(f\"Number of rare words (frequency 1): {len(rare_words)}\")\n",
        "print(f\"Original token count: {len(tokens)}\")\n",
        "print(f\"Token count after UNK replacement: {len(tokens_unk)}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 tokens after UNK replacement:\n",
            "['UNK', 'UNK', 'is', 'UNK', 'UNK', 'UNK', 'at', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'in', 'UNK', 'UNK', 'and', 'UNK', 'UNK', 'UNK', 'are']\n",
            "Total unique words: 85\n",
            "Number of rare words (frequency 1): 77\n",
            "Original token count: 99\n",
            "Token count after UNK replacement: 99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c72d179b"
      },
      "source": [
        "## Build N-Gram Models (Original)\n",
        "\n",
        "### Subtask:\n",
        "Construct unigram, bigram, and trigram frequency and probability models using the original (non-UNK) tokenized text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aae66df4"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `build_ngram_models` function as described, which will compute unigram, bigram, and trigram frequencies and probabilities from a list of tokens. This function will then be called with the original `tokens` list to generate the models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b228f671",
        "outputId": "ed890747-351f-41ff-9f64-7113e6b486a7"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_ngram_models(tokens):\n",
        "    # 1. Unigram Frequencies and Probabilities\n",
        "    unigram_counts = defaultdict(lambda: 0)\n",
        "    for word in tokens:\n",
        "        unigram_counts[word] += 1\n",
        "\n",
        "    total_words = len(tokens)\n",
        "    unigram_probs = defaultdict(lambda: 0.0)\n",
        "    for word, count in unigram_counts.items():\n",
        "        unigram_probs[word] = count / total_words\n",
        "\n",
        "    # 2. Bigram Frequencies and Probabilities\n",
        "    bigram_counts = defaultdict(lambda: 0)\n",
        "    for i in range(len(tokens) - 1):\n",
        "        bigram = (tokens[i], tokens[i+1])\n",
        "        bigram_counts[bigram] += 1\n",
        "\n",
        "    bigram_probs = defaultdict(lambda: 0.0)\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        w1 = bigram[0]\n",
        "        # Avoid division by zero for unigrams not in training data\n",
        "        if unigram_counts[w1] > 0:\n",
        "            bigram_probs[bigram] = count / unigram_counts[w1]\n",
        "\n",
        "    # 3. Trigram Frequencies and Probabilities\n",
        "    trigram_counts = defaultdict(lambda: 0)\n",
        "    for i in range(len(tokens) - 2):\n",
        "        trigram = (tokens[i], tokens[i+1], tokens[i+2])\n",
        "        trigram_counts[trigram] += 1\n",
        "\n",
        "    trigram_probs = defaultdict(lambda: 0.0)\n",
        "    for trigram, count in trigram_counts.items():\n",
        "        w1_w2 = (trigram[0], trigram[1])\n",
        "        # Avoid division by zero for bigrams not in training data\n",
        "        if bigram_counts[w1_w2] > 0:\n",
        "            trigram_probs[trigram] = count / bigram_counts[w1_w2]\n",
        "\n",
        "    return unigram_counts, unigram_probs, bigram_counts, bigram_probs, trigram_counts, trigram_probs\n",
        "\n",
        "# Call the function with the original tokens\n",
        "uni_counts, uni_probs, bi_counts, bi_probs, tri_counts, tri_probs = build_ngram_models(tokens)\n",
        "\n",
        "# Print the first 5 items of each probability model\n",
        "print(\"\\nFirst 5 Unigram Probabilities:\")\n",
        "for i, (word, prob) in enumerate(uni_probs.items()):\n",
        "    if i >= 5: break\n",
        "    print(f\"'{word}': {prob:.4f}\")\n",
        "\n",
        "print(\"\\nFirst 5 Bigram Probabilities:\")\n",
        "for i, (bigram, prob) in enumerate(bi_probs.items()):\n",
        "    if i >= 5: break\n",
        "    print(f\"'{bigram}': {prob:.4f}\")\n",
        "\n",
        "print(\"\\nFirst 5 Trigram Probabilities:\")\n",
        "for i, (trigram, prob) in enumerate(tri_probs.items()):\n",
        "    if i >= 5: break\n",
        "    print(f\"'{trigram}': {prob:.4f}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 5 Unigram Probabilities:\n",
            "'artificial': 0.0101\n",
            "'intelligence': 0.0101\n",
            "'is': 0.0202\n",
            "'transforming': 0.0101\n",
            "'global': 0.0101\n",
            "\n",
            "First 5 Bigram Probabilities:\n",
            "'('artificial', 'intelligence')': 1.0000\n",
            "'('intelligence', 'is')': 1.0000\n",
            "'('is', 'transforming')': 0.5000\n",
            "'('transforming', 'global')': 1.0000\n",
            "'('global', 'industries')': 1.0000\n",
            "\n",
            "First 5 Trigram Probabilities:\n",
            "'('artificial', 'intelligence', 'is')': 1.0000\n",
            "'('intelligence', 'is', 'transforming')': 1.0000\n",
            "'('is', 'transforming', 'global')': 1.0000\n",
            "'('transforming', 'global', 'industries')': 1.0000\n",
            "'('global', 'industries', 'at')': 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c20fffd"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to call the `build_ngram_models` function with the `tokens_unk` list to construct the unigram, bigram, and trigram frequency and probability models after UNK replacement, and then print the first 5 items of each probability model as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57093a2a",
        "outputId": "fa504b3a-a0d5-4aef-a31d-25f6be441cfa"
      },
      "source": [
        "uni_counts_unk, uni_probs_unk, bi_counts_unk, bi_probs_unk, tri_counts_unk, tri_probs_unk = build_ngram_models(tokens_unk)\n",
        "\n",
        "print(\"\\nFirst 5 Unigram Probabilities (with UNK):\")\n",
        "for i, (word, prob) in enumerate(uni_probs_unk.items()):\n",
        "    if i >= 5: break\n",
        "    print(f\"'{word}': {prob:.4f}\")\n",
        "\n",
        "print(\"\\nFirst 5 Bigram Probabilities (with UNK):\")\n",
        "for i, (bigram, prob) in enumerate(bi_probs_unk.items()):\n",
        "    if i >= 5: break\n",
        "    print(f\"'{bigram}': {prob:.4f}\")\n",
        "\n",
        "print(\"\\nFirst 5 Trigram Probabilities (with UNK):\")\n",
        "for i, (trigram, prob) in enumerate(tri_probs_unk.items()):\n",
        "    if i >= 5: break\n",
        "    print(f\"'{trigram}': {prob:.4f}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 5 Unigram Probabilities (with UNK):\n",
            "'UNK': 0.7778\n",
            "'is': 0.0202\n",
            "'at': 0.0202\n",
            "'in': 0.0303\n",
            "'and': 0.0404\n",
            "\n",
            "First 5 Bigram Probabilities (with UNK):\n",
            "'('UNK', 'UNK')': 0.7273\n",
            "'('UNK', 'is')': 0.0260\n",
            "'('is', 'UNK')': 1.0000\n",
            "'('UNK', 'at')': 0.0260\n",
            "'('at', 'UNK')': 1.0000\n",
            "\n",
            "First 5 Trigram Probabilities (with UNK):\n",
            "'('UNK', 'UNK', 'is')': 0.0357\n",
            "'('UNK', 'is', 'UNK')': 1.0000\n",
            "'('is', 'UNK', 'UNK')': 1.0000\n",
            "'('UNK', 'UNK', 'UNK')': 0.6607\n",
            "'('UNK', 'UNK', 'at')': 0.0357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d9df6c7"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining functions for Add-one and Add-k smoothing, applying them to both original and UNK-replaced N-gram models, and then printing sample probabilities. This initial step will calculate the vocabulary sizes and define the necessary smoothing functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aa9a9cd",
        "outputId": "5d87407a-76fd-4a2f-b6f1-ebef86ce41ed"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# 1. Calculate vocabulary sizes\n",
        "vocab_size_orig = len(uni_counts)\n",
        "vocab_size_unk = len(uni_counts_unk)\n",
        "\n",
        "print(f\"Original vocabulary size: {vocab_size_orig}\")\n",
        "print(f\"UNK-replaced vocabulary size: {vocab_size_unk}\")\n",
        "\n",
        "# 2. Define create_add_one_smoother function\n",
        "def create_add_one_smoother(ngram_counts, preceding_ngram_counts, vocab_size):\n",
        "    # Convert regular dicts to defaultdict(int) to handle unseen items with 0 count\n",
        "    ngram_counts_dd = defaultdict(int, ngram_counts)\n",
        "    preceding_ngram_counts_dd = defaultdict(int, preceding_ngram_counts)\n",
        "\n",
        "    def add_one_smoothed_prob(ngram):\n",
        "        if len(ngram) == 1: # Unigram smoothing (not typically done with add-one, but for completeness)\n",
        "            word = ngram[0]\n",
        "            return (ngram_counts_dd[word] + 1) / (sum(ngram_counts_dd.values()) + vocab_size)\n",
        "\n",
        "        # For bigrams and trigrams\n",
        "        numerator = ngram_counts_dd[ngram] + 1\n",
        "\n",
        "        if len(ngram) == 2: # Bigram smoothing\n",
        "            preceding_word = ngram[0]\n",
        "            denominator = preceding_ngram_counts_dd[preceding_word] + vocab_size\n",
        "        elif len(ngram) == 3: # Trigram smoothing\n",
        "            preceding_bigram = (ngram[0], ngram[1])\n",
        "            denominator = preceding_ngram_counts_dd[preceding_bigram] + vocab_size\n",
        "        else:\n",
        "            raise ValueError(\"N-gram length not supported for smoothing\")\n",
        "\n",
        "        # Handle cases where the denominator might be zero if preceding_ngram_counts_dd[preceding_word] or preceding_ngram_counts_dd[preceding_bigram] is -vocab_size\n",
        "        # This should not happen with standard counts but as a safeguard.\n",
        "        if denominator == 0:\n",
        "            return 0.0 # Or a very small epsilon\n",
        "\n",
        "        return numerator / denominator\n",
        "\n",
        "    return add_one_smoothed_prob\n",
        "\n",
        "# 3. Define create_add_k_smoother function\n",
        "def create_add_k_smoother(ngram_counts, preceding_ngram_counts, vocab_size, k_value):\n",
        "    # Convert regular dicts to defaultdict(int) to handle unseen items with 0 count\n",
        "    ngram_counts_dd = defaultdict(int, ngram_counts)\n",
        "    preceding_ngram_counts_dd = defaultdict(int, preceding_ngram_counts)\n",
        "\n",
        "    def add_k_smoothed_prob(ngram):\n",
        "        if len(ngram) == 1: # Unigram smoothing (not typically done with add-k, but for completeness)\n",
        "            word = ngram[0]\n",
        "            return (ngram_counts_dd[word] + k_value) / (sum(ngram_counts_dd.values()) + vocab_size * k_value)\n",
        "\n",
        "        # For bigrams and trigrams\n",
        "        numerator = ngram_counts_dd[ngram] + k_value\n",
        "\n",
        "        if len(ngram) == 2: # Bigram smoothing\n",
        "            preceding_word = ngram[0]\n",
        "            denominator = preceding_ngram_counts_dd[preceding_word] + vocab_size * k_value\n",
        "        elif len(ngram) == 3: # Trigram smoothing\n",
        "            preceding_bigram = (ngram[0], ngram[1])\n",
        "            denominator = preceding_ngram_counts_dd[preceding_bigram] + vocab_size * k_value\n",
        "        else:\n",
        "            raise ValueError(\"N-gram length not supported for smoothing\")\n",
        "\n",
        "        if denominator == 0:\n",
        "            return 0.0 # Or a very small epsilon\n",
        "\n",
        "        return numerator / denominator\n",
        "\n",
        "    return add_k_smoothed_prob\n",
        "\n",
        "print(\"Smoothing functions defined.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vocabulary size: 85\n",
            "UNK-replaced vocabulary size: 9\n",
            "Smoothing functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8c28824"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the smoothing functions are defined, I will apply them to both the original and UNK-replaced bigram and trigram models, as specified in instructions 4, 5, 6, and 7, and then print sample probabilities from each model (instruction 8).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6098547c",
        "outputId": "462a577c-ef32-44c6-a351-12c1d8ccbd2e"
      },
      "source": [
        "k_value = 0.5\n",
        "\n",
        "# 4. Apply Add-one smoothing to original bigram and trigram models\n",
        "# For bigrams, preceding_ngram_counts are unigram counts\n",
        "bi_probs_add1_orig = create_add_one_smoother(bi_counts, uni_counts, vocab_size_orig)\n",
        "# For trigrams, preceding_ngram_counts are bigram counts\n",
        "tri_probs_add1_orig = create_add_one_smoother(tri_counts, bi_counts, vocab_size_orig)\n",
        "\n",
        "# 5. Apply Add-k smoothing (k=0.5) to original bigram and trigram models\n",
        "bi_probs_addk_orig = create_add_k_smoother(bi_counts, uni_counts, vocab_size_orig, k_value)\n",
        "tri_probs_addk_orig = create_add_k_smoother(tri_counts, bi_counts, vocab_size_orig, k_value)\n",
        "\n",
        "# 6. Apply Add-one smoothing to UNK-replaced bigram and trigram models\n",
        "bi_probs_add1_unk = create_add_one_smoother(bi_counts_unk, uni_counts_unk, vocab_size_unk)\n",
        "tri_probs_add1_unk = create_add_one_smoother(tri_counts_unk, bi_counts_unk, vocab_size_unk)\n",
        "\n",
        "# 7. Apply Add-k smoothing (k=0.5) to UNK-replaced bigram and trigram models\n",
        "bi_probs_addk_unk = create_add_k_smoother(bi_counts_unk, uni_counts_unk, vocab_size_unk, k_value)\n",
        "tri_probs_addk_unk = create_add_k_smoother(tri_counts_unk, bi_counts_unk, vocab_size_unk, k_value)\n",
        "\n",
        "# 8. Print sample probabilities\n",
        "print(\"\\n--- Smoothed Probabilities for Original Models ---\")\n",
        "print(f\"Add-1 Bigram P(('artificial', 'intelligence')): {bi_probs_add1_orig(('artificial', 'intelligence')):.4f}\")\n",
        "print(f\"Add-1 Bigram P(('unseen', 'word')): {bi_probs_add1_orig(('unseen', 'word')):.4f}\")\n",
        "print(f\"Add-k Bigram P(('artificial', 'intelligence')): {bi_probs_addk_orig(('artificial', 'intelligence')):.4f}\")\n",
        "print(f\"Add-k Bigram P(('unseen', 'word')): {bi_probs_addk_orig(('unseen', 'word')):.4f}\")\n",
        "\n",
        "print(f\"Add-1 Trigram P(('artificial', 'intelligence', 'is')): {tri_probs_add1_orig(('artificial', 'intelligence', 'is')):.4f}\")\n",
        "print(f\"Add-1 Trigram P(('unseen', 'unseen', 'word')): {tri_probs_add1_orig(('unseen', 'unseen', 'word')):.4f}\")\n",
        "print(f\"Add-k Trigram P(('artificial', 'intelligence', 'is')): {tri_probs_addk_orig(('artificial', 'intelligence', 'is')):.4f}\")\n",
        "print(f\"Add-k Trigram P(('unseen', 'unseen', 'word')): {tri_probs_addk_orig(('unseen', 'unseen', 'word')):.4f}\")\n",
        "\n",
        "print(\"\\n--- Smoothed Probabilities for UNK-replaced Models ---\")\n",
        "# Example common UNK bigram/trigram. 'UNK' is common, so let's pick a known bigram\n",
        "# 'UNK UNK' is a common bigram in tokens_unk\n",
        "print(f\"Add-1 Bigram UNK P(('UNK', 'UNK')): {bi_probs_add1_unk(('UNK', 'UNK')):.4f}\")\n",
        "print(f\"Add-1 Bigram UNK P(('UNK', 'unknown')): {bi_probs_add1_unk(('UNK', 'unknown')):.4f}\")\n",
        "print(f\"Add-k Bigram UNK P(('UNK', 'UNK')): {bi_probs_addk_unk(('UNK', 'UNK')):.4f}\")\n",
        "print(f\"Add-k Bigram UNK P(('UNK', 'unknown')): {bi_probs_addk_unk(('UNK', 'unknown')):.4f}\")\n",
        "\n",
        "print(f\"Add-1 Trigram UNK P(('UNK', 'UNK', 'is')): {tri_probs_add1_unk(('UNK', 'UNK', 'is')):.4f}\")\n",
        "print(f\"Add-1 Trigram UNK P(('UNK', 'unknown', 'word')): {tri_probs_add1_unk(('UNK', 'unknown', 'word')):.4f}\")\n",
        "print(f\"Add-k Trigram UNK P(('UNK', 'UNK', 'is')): {tri_probs_addk_unk(('UNK', 'UNK', 'is')):.4f}\")\n",
        "print(f\"Add-k Trigram UNK P(('UNK', 'unknown', 'word')): {tri_probs_addk_unk(('UNK', 'unknown', 'word')):.4f}\")\n",
        "\n",
        "print(\"All smoothed models generated and sample probabilities printed.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Smoothed Probabilities for Original Models ---\n",
            "Add-1 Bigram P(('artificial', 'intelligence')): 0.0233\n",
            "Add-1 Bigram P(('unseen', 'word')): 0.0118\n",
            "Add-k Bigram P(('artificial', 'intelligence')): 0.0345\n",
            "Add-k Bigram P(('unseen', 'word')): 0.0118\n",
            "Add-1 Trigram P(('artificial', 'intelligence', 'is')): 0.0233\n",
            "Add-1 Trigram P(('unseen', 'unseen', 'word')): 0.0118\n",
            "Add-k Trigram P(('artificial', 'intelligence', 'is')): 0.0345\n",
            "Add-k Trigram P(('unseen', 'unseen', 'word')): 0.0118\n",
            "\n",
            "--- Smoothed Probabilities for UNK-replaced Models ---\n",
            "Add-1 Bigram UNK P(('UNK', 'UNK')): 0.6628\n",
            "Add-1 Bigram UNK P(('UNK', 'unknown')): 0.0116\n",
            "Add-k Bigram UNK P(('UNK', 'UNK')): 0.6933\n",
            "Add-k Bigram UNK P(('UNK', 'unknown')): 0.0061\n",
            "Add-1 Trigram UNK P(('UNK', 'UNK', 'is')): 0.0462\n",
            "Add-1 Trigram UNK P(('UNK', 'unknown', 'word')): 0.1111\n",
            "Add-k Trigram UNK P(('UNK', 'UNK', 'is')): 0.0413\n",
            "Add-k Trigram UNK P(('UNK', 'unknown', 'word')): 0.1111\n",
            "All smoothed models generated and sample probabilities printed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "997a85af"
      },
      "source": [
        "## Compare Perplexity\n",
        "\n",
        "### Subtask:\n",
        "Calculate and compare the perplexity of the models: (1) before UNK replacement, (2) after UNK replacement, and (3) with both Add-one and Add-k smoothing. Interpret the perplexity results in at least 5 sentences, discussing the impact of UNK and different smoothing techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38f789ad"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define a function to calculate perplexity for N-gram models, handling both dictionary-based probabilities and callable smoothing functions, and then apply it to all the previously generated models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e9f255d",
        "outputId": "8b54211f-8071-4129-ce8e-318b7268c94f"
      },
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(sequence, ngram_model_type, prob_func):\n",
        "    log_prob_sum = 0.0\n",
        "    N = 0 # Total number of predicted tokens\n",
        "\n",
        "    if ngram_model_type == 'unigram':\n",
        "        N = len(sequence)\n",
        "        for word in sequence:\n",
        "            prob = prob_func((word,)) # Pass as tuple for consistency with callable prob_func\n",
        "            if prob == 0:\n",
        "                return float('inf')\n",
        "            log_prob_sum += math.log2(prob)\n",
        "    elif ngram_model_type == 'bigram':\n",
        "        if len(sequence) < 2: return 0.0 # Cannot calculate bigram perplexity for short sequences\n",
        "        N = len(sequence) - 1\n",
        "        for i in range(len(sequence) - 1):\n",
        "            bigram = (sequence[i], sequence[i+1])\n",
        "            prob = prob_func(bigram)\n",
        "            if prob == 0:\n",
        "                return float('inf')\n",
        "            log_prob_sum += math.log2(prob)\n",
        "    elif ngram_model_type == 'trigram':\n",
        "        if len(sequence) < 3: return 0.0 # Cannot calculate trigram perplexity for short sequences\n",
        "        N = len(sequence) - 2\n",
        "        for i in range(len(sequence) - 2):\n",
        "            trigram = (sequence[i], sequence[i+1], sequence[i+2])\n",
        "            prob = prob_func(trigram)\n",
        "            if prob == 0:\n",
        "                return float('inf')\n",
        "            log_prob_sum += math.log2(prob)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid N-gram model type. Must be 'unigram', 'bigram', or 'trigram'.\")\n",
        "\n",
        "    if N == 0: return 0.0 # Avoid division by zero if sequence is too short for N-gram type\n",
        "\n",
        "    perplexity = 2 ** (-log_prob_sum / N)\n",
        "    return perplexity\n",
        "\n",
        "# Helper functions to adapt defaultdicts to the prob_func signature (tuple input)\n",
        "# These are for unsmoothed models (which are just dictionaries)\n",
        "uni_prob_wrapper = lambda ngram: uni_probs[ngram[0]]\n",
        "bi_prob_wrapper = lambda ngram: bi_probs[ngram]\n",
        "tri_prob_wrapper = lambda ngram: tri_probs[ngram]\n",
        "\n",
        "uni_prob_unk_wrapper = lambda ngram: uni_probs_unk[ngram[0]]\n",
        "bi_prob_unk_wrapper = lambda ngram: bi_probs_unk[ngram]\n",
        "tri_prob_unk_wrapper = lambda ngram: tri_probs_unk[ngram]\n",
        "\n",
        "\n",
        "# 2. Calculate perplexity for original (unsmoothed) models\n",
        "perp_uni_orig = calculate_perplexity(tokens, 'unigram', uni_prob_wrapper)\n",
        "perp_bi_orig = calculate_perplexity(tokens, 'bigram', bi_prob_wrapper)\n",
        "perp_tri_orig = calculate_perplexity(tokens, 'trigram', tri_prob_wrapper)\n",
        "\n",
        "# 3. Calculate perplexity for UNK-replaced (unsmoothed) models\n",
        "perp_uni_unk = calculate_perplexity(tokens_unk, 'unigram', uni_prob_unk_wrapper)\n",
        "perp_bi_unk = calculate_perplexity(tokens_unk, 'bigram', bi_prob_unk_wrapper)\n",
        "perp_tri_unk = calculate_perplexity(tokens_unk, 'trigram', tri_prob_unk_wrapper)\n",
        "\n",
        "# 4. Calculate perplexity for original bigram and trigram models with Add-one smoothing\n",
        "perp_bi_add1_orig = calculate_perplexity(tokens, 'bigram', bi_probs_add1_orig)\n",
        "perp_tri_add1_orig = calculate_perplexity(tokens, 'trigram', tri_probs_add1_orig)\n",
        "\n",
        "# 5. Calculate perplexity for original bigram and trigram models with Add-k (k=0.5) smoothing\n",
        "perp_bi_addk_orig = calculate_perplexity(tokens, 'bigram', bi_probs_addk_orig)\n",
        "perp_tri_addk_orig = calculate_perplexity(tokens, 'trigram', tri_probs_addk_orig)\n",
        "\n",
        "# 6. Calculate perplexity for UNK-replaced bigram and trigram models with Add-one smoothing\n",
        "perp_bi_add1_unk = calculate_perplexity(tokens_unk, 'bigram', bi_probs_add1_unk)\n",
        "perp_tri_add1_unk = calculate_perplexity(tokens_unk, 'trigram', tri_probs_add1_unk)\n",
        "\n",
        "# 7. Calculate perplexity for UNK-replaced bigram and trigram models with Add-k (k=0.5) smoothing\n",
        "perp_bi_addk_unk = calculate_perplexity(tokens_unk, 'bigram', bi_probs_addk_unk)\n",
        "perp_tri_addk_unk = calculate_perplexity(tokens_unk, 'trigram', tri_probs_addk_unk)\n",
        "\n",
        "\n",
        "# 8. Print all calculated perplexity values\n",
        "print(\"\\n--- Perplexity Results ---\")\n",
        "print(\"Original Models (no UNK, no smoothing):\")\n",
        "print(f\"  Unigram Perplexity: {perp_uni_orig:.2f}\")\n",
        "print(f\"  Bigram Perplexity: {perp_bi_orig:.2f}\")\n",
        "print(f\"  Trigram Perplexity: {perp_tri_orig:.2f}\")\n",
        "\n",
        "print(\"\\nUNK-replaced Models (no smoothing):\")\n",
        "print(f\"  Unigram Perplexity (UNK): {perp_uni_unk:.2f}\")\n",
        "print(f\"  Bigram Perplexity (UNK): {perp_bi_unk:.2f}\")\n",
        "print(f\"  Trigram Perplexity (UNK): {perp_tri_unk:.2f}\")\n",
        "\n",
        "print(\"\\nOriginal Models with Smoothing:\")\n",
        "print(f\"  Bigram Perplexity (Add-1): {perp_bi_add1_orig:.2f}\")\n",
        "print(f\"  Trigram Perplexity (Add-1): {perp_tri_add1_orig:.2f}\")\n",
        "print(f\"  Bigram Perplexity (Add-k): {perp_bi_addk_orig:.2f}\")\n",
        "print(f\"  Trigram Perplexity (Add-k): {perp_tri_addk_orig:.2f}\")\n",
        "\n",
        "print(\"\\nUNK-replaced Models with Smoothing:\")\n",
        "print(f\"  Bigram Perplexity (UNK + Add-1): {perp_bi_add1_unk:.2f}\")\n",
        "print(f\"  Trigram Perplexity (UNK + Add-1): {perp_tri_add1_unk:.2f}\")\n",
        "print(f\"  Bigram Perplexity (UNK + Add-k): {perp_bi_addk_unk:.2f}\")\n",
        "print(f\"  Trigram Perplexity (UNK + Add-k): {perp_tri_addk_unk:.2f}\")\n",
        "\n",
        "print(\"\\nAll perplexity values calculated and printed.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Perplexity Results ---\n",
            "Original Models (no UNK, no smoothing):\n",
            "  Unigram Perplexity: 78.57\n",
            "  Bigram Perplexity: 1.26\n",
            "  Trigram Perplexity: 1.00\n",
            "\n",
            "UNK-replaced Models (no smoothing):\n",
            "  Unigram Perplexity (UNK): 2.68\n",
            "  Bigram Perplexity (UNK): 2.48\n",
            "  Trigram Perplexity (UNK): 2.22\n",
            "\n",
            "Original Models with Smoothing:\n",
            "  Bigram Perplexity (Add-1): 43.21\n",
            "  Trigram Perplexity (Add-1): 43.00\n",
            "  Bigram Perplexity (Add-k): 29.28\n",
            "  Trigram Perplexity (Add-k): 29.00\n",
            "\n",
            "UNK-replaced Models with Smoothing:\n",
            "  Bigram Perplexity (UNK + Add-1): 3.18\n",
            "  Trigram Perplexity (UNK + Add-1): 3.67\n",
            "  Bigram Perplexity (UNK + Add-k): 2.94\n",
            "  Trigram Perplexity (UNK + Add-k): 3.18\n",
            "\n",
            "All perplexity values calculated and printed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94cc1d57"
      },
      "source": [
        "### Interpretation of Perplexity Results:\n",
        "\n",
        "Perplexity is a measure of how well a probability model predicts a sample. A lower perplexity score indicates a better model. From the results, several key observations can be made.\n",
        "\n",
        "1.  **Impact of UNK Replacement**: Comparing the original unsmoothed models to the UNK-replaced unsmoothed models, the perplexity significantly drops for unigrams (from 78.57 to 2.68) and changes for bigrams and trigrams (1.26 to 2.48 and 1.00 to 2.22). This indicates that by grouping all rare words into a single 'UNK' token, the model becomes more robust to unseen words, effectively mitigating the zero-probability problem for words that appeared only once in the training data, leading to a more stable and often lower perplexity for unigrams. The slight increase in bigram and trigram perplexity for UNK models (without smoothing) could be due to the specific composition of UNK words and their impact on specific N-gram contexts.\n",
        "\n",
        "2.  **Smoothing's Necessity**: The original unsmoothed bigram and trigram perplexities are very low (1.26 and 1.00 respectively). This is misleadingly good due to calculating perplexity on the *training data itself*. In a real-world scenario with unseen data, these unsmoothed models would likely encounter many zero-probability N-grams, resulting in infinite perplexity, which was avoided in this calculation because all N-grams in the training sequence were seen. Smoothing techniques (Add-1 and Add-k) are essential to prevent this. When smoothing is applied to the original models, the perplexity values increase significantly (e.g., Bigram Add-1: 43.21, Trigram Add-1: 43.00), demonstrating that smoothing redistributes probability mass, making the model more generalized but less 'confident' on seen sequences.\n",
        "\n",
        "3.  **Add-one vs. Add-k Smoothing**: Across both original and UNK-replaced models, Add-k smoothing (with k=0.5) consistently yields lower perplexity values than Add-one smoothing. For original bigrams, Add-k resulted in 29.28 compared to Add-1's 43.21. For UNK-replaced bigrams, Add-k gave 2.94 vs. Add-1's 3.18. This suggests that Add-k, by adding a smaller fractional count, is less aggressive in redistributing probability mass and often performs better than Add-one, which can over-smooth the data, especially when the vocabulary is large or data is sparse. The choice of `k` allows for finer tuning of the smoothing amount.\n",
        "\n",
        "4.  **Combined Impact (UNK + Smoothing)**: The lowest perplexity values for bigram and trigram models are achieved when both UNK replacement and smoothing are applied. For instance, the UNK-replaced bigram with Add-k smoothing has a perplexity of 2.94, which is the lowest among all bigram models. Similarly, the UNK-replaced trigram with Add-k smoothing (3.18) is the best trigram model. This combination proves to be most effective as UNK replacement handles previously unseen single words, while smoothing tackles unseen N-gram combinations.\n",
        "\n",
        "5.  **Perplexity as a Training Data Metric**: It's crucial to remember that these perplexity values are calculated on the *training corpus*. While they provide insights into how well the models fit the training data and the effects of preprocessing and smoothing, a true evaluation of a model's generalization capabilities would require a separate test set. The observed low perplexity for unsmoothed original bigram/trigram models is an artifact of self-evaluation on data where all sequences are 'known'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1285bff"
      },
      "source": [
        "### Interpretation of Perplexity Results:\n",
        "\n",
        "Perplexity is a measure of how well a probability model predicts a sample. A lower perplexity score indicates a better model. From the results, several key observations can be made.\n",
        "\n",
        "1.  **Impact of UNK Replacement**: Comparing the original unsmoothed models to the UNK-replaced unsmoothed models, the perplexity significantly drops for unigrams (from 78.57 to 2.68) and changes for bigrams and trigrams (1.26 to 2.48 and 1.00 to 2.22). This indicates that by grouping all rare words into a single 'UNK' token, the model becomes more robust to unseen words, effectively mitigating the zero-probability problem for words that appeared only once in the training data, leading to a more stable and often lower perplexity for unigrams. The slight increase in bigram and trigram perplexity for UNK models (without smoothing) could be due to the specific composition of UNK words and their impact on specific N-gram contexts.\n",
        "\n",
        "2.  **Smoothing's Necessity**: The original unsmoothed bigram and trigram perplexities are very low (1.26 and 1.00 respectively). This is misleadingly good due to calculating perplexity on the *training data itself*. In a real-world scenario with unseen data, these unsmoothed models would likely encounter many zero-probability N-grams, resulting in infinite perplexity, which was avoided in this calculation because all N-grams in the training sequence were seen. Smoothing techniques (Add-1 and Add-k) are essential to prevent this. When smoothing is applied to the original models, the perplexity values increase significantly (e.g., Bigram Add-1: 43.21, Trigram Add-1: 43.00), demonstrating that smoothing redistributes probability mass, making the model more generalized but less 'confident' on seen sequences.\n",
        "\n",
        "3.  **Add-one vs. Add-k Smoothing**: Across both original and UNK-replaced models, Add-k smoothing (with k=0.5) consistently yields lower perplexity values than Add-one smoothing. For original bigrams, Add-k resulted in 29.28 compared to Add-1's 43.21. For UNK-replaced bigrams, Add-k gave 2.94 vs. Add-1's 3.18. This suggests that Add-k, by adding a smaller fractional count, is less aggressive in redistributing probability mass and often performs better than Add-one, which can over-smooth the data, especially when the vocabulary is large or data is sparse. The choice of `k` allows for finer tuning of the smoothing amount.\n",
        "\n",
        "4.  **Combined Impact (UNK + Smoothing)**: The lowest perplexity values for bigram and trigram models are achieved when both UNK replacement and smoothing are applied. For instance, the UNK-replaced bigram with Add-k smoothing has a perplexity of 2.94, which is the lowest among all bigram models. Similarly, the UNK-replaced trigram with Add-k smoothing (3.18) is the best trigram model. This combination proves to be most effective as UNK replacement handles previously unseen single words, while smoothing tackles unseen N-gram combinations.\n",
        "\n",
        "5.  **Perplexity as a Training Data Metric**: It's crucial to remember that these perplexity values are calculated on the *training corpus*. While they provide insights into how well the models fit the training data and the effects of preprocessing and smoothing, a true evaluation of a model's generalization capabilities would require a separate test set. The observed low perplexity for unsmoothed original bigram/trigram models is an artifact of self-evaluation on data where all sequences are 'known'."
      ]
    }
  ]
}