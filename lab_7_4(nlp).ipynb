{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaVQuSZdUcoqXJe46ESyri",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaya-02ly/akshaya2260-nlp/blob/main/lab_7_4(nlp).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5107edfd"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "### Subtask:\n",
        "Import necessary libraries for text preprocessing, similarity calculations, and WordNet semantic similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c0f4011"
      },
      "source": [
        "### Question 7: Give two real-life applications of text similarity.\n",
        "\n",
        "Text similarity is a fundamental concept in NLP with numerous real-life applications across various domains. Here are two prominent examples:\n",
        "\n",
        "1.  **Plagiarism Detection**: Text similarity is extensively used in educational institutions and publishing houses to identify instances of plagiarism. By comparing a submitted document (e.g., a student essay, a research paper) against a vast database of existing texts (e.g., academic databases, web content), similarity algorithms can detect passages or entire documents that have been copied or paraphrased without proper attribution. Tools like Turnitin or Grammarly utilize sophisticated text similarity techniques to highlight overlapping content, helping to maintain academic integrity and intellectual property rights.\n",
        "    *   **How it works**: Documents are broken down into smaller units (sentences, paragraphs), converted into numerical representations (e.g., TF-IDF vectors, embeddings), and then compared using metrics like Cosine or Jaccard similarity. High similarity scores between a submitted text and a source text indicate potential plagiarism.\n",
        "\n",
        "2.  **Recommendation Systems (Content-Based Filtering)**: Many online platforms use text similarity to power their recommendation engines, particularly in content-based filtering. This involves recommending items to users based on the similarity between the item's description and the user's past preferences or the descriptions of items they have previously liked. Examples include recommending:\n",
        "    *   **Movies/TV Shows**: Based on plot summaries, genres, and tags.\n",
        "    *   **News Articles**: Based on the content of articles a user has read before.\n",
        "    *   **Products**: Based on product descriptions that match items a user has browsed or purchased.\n",
        "    *   **Academic Papers**: Based on abstracts and keywords of papers relevant to a researcher's interests.\n",
        "    *   **How it works**: Text descriptions of items are transformed into numerical vectors. When a user expresses interest in an item, its vector is compared (using cosine similarity, for instance) with the vectors of other available items to find the most similar ones. The items with the highest similarity scores are then recommended to the user.\n",
        "\n",
        "These applications demonstrate how quantifying text similarity enables automated systems to process, understand, and leverage textual information to provide valuable services and insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15114af4"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries for text preprocessing, feature extraction, similarity calculation, and numerical operations as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aafd4a76"
      },
      "source": [
        "import nltk # For natural language processing tasks like tokenization, stopword removal, lemmatization, and WordNet.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # To convert text into numerical feature vectors.\n",
        "from sklearn.metrics.pairwise import cosine_similarity # To compute cosine similarity between text representations.\n",
        "import numpy as np # For numerical operations."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58e1ff8b"
      },
      "source": [
        "## Load/Prepare Dataset\n",
        "\n",
        "### Subtask:\n",
        "Create a sample dataset of sentence pairs, either manually or by loading from a file (e.g., .txt or .csv). Display a sample of the dataset and provide a 5-6 line explanation of its structure and content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cedae1c"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a sample dataset of sentences as a Python list named `corpus` for text similarity analysis, as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1d72ea6",
        "outputId": "e7918722-97fc-4349-dad8-f3141f85271a"
      },
      "source": [
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A fast brown fox leaps over a sleepy dog.\",\n",
        "    \"The cat sits on the mat.\",\n",
        "    \"Dogs are loyal pets and great companions.\",\n",
        "    \"Felines are often kept as domestic animals.\",\n",
        "    \"The weather today is sunny and warm.\",\n",
        "    \"It is a beautiful and hot day outside.\"\n",
        "]\n",
        "\n",
        "print(\"Sample Corpus:\")\n",
        "for i, sentence in enumerate(corpus):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Corpus:\n",
            "Sentence 1: The quick brown fox jumps over the lazy dog.\n",
            "Sentence 2: A fast brown fox leaps over a sleepy dog.\n",
            "Sentence 3: The cat sits on the mat.\n",
            "Sentence 4: Dogs are loyal pets and great companions.\n",
            "Sentence 5: Felines are often kept as domestic animals.\n",
            "Sentence 6: The weather today is sunny and warm.\n",
            "Sentence 7: It is a beautiful and hot day outside.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f14dbef7"
      },
      "source": [
        "```markdown\n",
        "The `corpus` is a Python list consisting of seven strings, where each string represents a distinct sentence or document.\n",
        "\n",
        "The sentences cover a variety of topics and demonstrate different levels of similarity. For instance, sentences 1 and 2 are semantically very similar, both describing a fox and a dog. Sentences 4 and 5 are also related, discussing domestic animals (dogs and felines). Sentences 3, 6, and 7, on the other hand, introduce entirely different themes (a cat on a mat, and weather descriptions). This diverse set allows for a comprehensive analysis of text similarity, comparing highly similar, moderately similar, and entirely dissimilar text pairs.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2560ac5"
      },
      "source": [
        "The `corpus` is a Python list consisting of seven strings, where each string represents a distinct sentence or document.\n",
        "\n",
        "The sentences cover a variety of topics and demonstrate different levels of similarity. For instance, sentences 1 and 2 are semantically very similar, both describing a fox and a dog. Sentences 4 and 5 are also related, discussing domestic animals (dogs and felines). Sentences 3, 6, and 7, on the other hand, introduce entirely different themes (a cat on a mat, and weather descriptions). This diverse set allows for a comprehensive analysis of text similarity, comparing highly similar, moderately similar, and entirely dissimilar text pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de96213b"
      },
      "source": [
        "## Preprocess Text\n",
        "\n",
        "### Subtask:\n",
        "Write and apply functions to preprocess the text data. This includes lowercasing, removing punctuation and numbers, removing stopwords, and tokenizing. Optionally, include lemmatization. Each preprocessing step should be clearly explained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14ccfb72"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin text preprocessing, it's essential to download the necessary NLTK data, including 'stopwords', 'wordnet', and 'punkt', which are crucial for subsequent steps like tokenization, stopword removal, and lemmatization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8d36f91",
        "outputId": "c8d5b94d-1837-4415-9cfa-158530b85bca"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4') # Open Multilingual Wordnet, often needed for WordNetLemmatizer\n",
        "\n",
        "print(\"NLTK data downloaded successfully.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK data downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d55743d9"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the NLTK data is downloaded, I will define the `preprocess_text` function, incorporating lowercasing, punctuation and number removal, tokenization, stopword removal, and lemmatization as specified in the instructions. Then I will apply this function to the corpus and display the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc729c0b",
        "outputId": "358b3e94-d5b1-4bd9-a6b4-32228aa35ca2"
      },
      "source": [
        "import string # For accessing punctuation characters.\n",
        "import re # For regular expression operations, specifically to remove numbers.\n",
        "from nltk.corpus import stopwords # For accessing a list of common English stopwords.\n",
        "from nltk.tokenize import word_tokenize # For splitting text into words.\n",
        "from nltk.stem import WordNetLemmatizer # For reducing words to their base form.\n",
        "\n",
        "# Initialize WordNetLemmatizer and stopwords set\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 1. Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 3. Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 4. Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 5. Remove stopwords and apply lemmatization\n",
        "    processed_tokens = []\n",
        "    for word in tokens:\n",
        "        if word not in stop_words:\n",
        "            processed_tokens.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    # 6. Join processed words back into a string\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "# Apply the preprocessing function to the corpus\n",
        "preprocessed_corpus = [preprocess_text(sentence) for sentence in corpus]\n",
        "\n",
        "# Print original and preprocessed versions of the first three sentences\n",
        "print(\"Original vs. Preprocessed Sentences (First 3):\")\n",
        "for i in range(min(3, len(corpus))):\n",
        "    print(f\"Original {i+1}: {corpus[i]}\")\n",
        "    print(f\"Preprocessed {i+1}: {preprocessed_corpus[i]}\\n\")\n",
        "\n",
        "print(\"Text preprocessing complete. 'preprocessed_corpus' created.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vs. Preprocessed Sentences (First 3):\n",
            "Original 1: The quick brown fox jumps over the lazy dog.\n",
            "Preprocessed 1: quick brown fox jump lazy dog\n",
            "\n",
            "Original 2: A fast brown fox leaps over a sleepy dog.\n",
            "Preprocessed 2: fast brown fox leap sleepy dog\n",
            "\n",
            "Original 3: The cat sits on the mat.\n",
            "Preprocessed 3: cat sits mat\n",
            "\n",
            "Text preprocessing complete. 'preprocessed_corpus' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f10ccc9f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the 'punkt_tab' resource, required by `word_tokenize`, was not found, despite `nltk.download('punkt')` being executed. To resolve this, I will explicitly add `nltk.download('punkt_tab')` to the NLTK data download step to ensure all necessary tokenization resources are available before text preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbb96016",
        "outputId": "f963338a-06a9-406a-9882-a01c402c17d8"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4') # Open Multilingual Wordnet, often needed for WordNetLemmatizer\n",
        "nltk.download('punkt_tab') # Explicitly download punkt_tab as it was reported missing\n",
        "\n",
        "print(\"NLTK data downloaded successfully.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK data downloaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26af84ef"
      },
      "source": [
        "**Reasoning**:\n",
        "With all NLTK resources now downloaded and available, I will re-attempt to define the `preprocess_text` function and apply it to the `corpus` to create the `preprocessed_corpus`, then display the first three original and preprocessed sentences to verify the preprocessing steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "228c6478",
        "outputId": "585d9623-f695-4732-9973-02c2c20a8db4"
      },
      "source": [
        "import string # For accessing punctuation characters.\n",
        "import re # For regular expression operations, specifically to remove numbers.\n",
        "from nltk.corpus import stopwords # For accessing a list of common English stopwords.\n",
        "from nltk.tokenize import word_tokenize # For splitting text into words.\n",
        "from nltk.stem import WordNetLemmatizer # For reducing words to their base form.\n",
        "\n",
        "# Initialize WordNetLemmatizer and stopwords set\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 1. Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 3. Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 4. Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 5. Remove stopwords and apply lemmatization\n",
        "    processed_tokens = []\n",
        "    for word in tokens:\n",
        "        if word not in stop_words:\n",
        "            processed_tokens.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    # 6. Join processed words back into a string\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "# Apply the preprocessing function to the corpus\n",
        "preprocessed_corpus = [preprocess_text(sentence) for sentence in corpus]\n",
        "\n",
        "# Print original and preprocessed versions of the first three sentences\n",
        "print(\"Original vs. Preprocessed Sentences (First 3):\")\n",
        "for i in range(min(3, len(corpus))):\n",
        "    print(f\"Original {i+1}: {corpus[i]}\")\n",
        "    print(f\"Preprocessed {i+1}: {preprocessed_corpus[i]}\\n\")\n",
        "\n",
        "print(\"Text preprocessing complete. 'preprocessed_corpus' created.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vs. Preprocessed Sentences (First 3):\n",
            "Original 1: The quick brown fox jumps over the lazy dog.\n",
            "Preprocessed 1: quick brown fox jump lazy dog\n",
            "\n",
            "Original 2: A fast brown fox leaps over a sleepy dog.\n",
            "Preprocessed 2: fast brown fox leap sleepy dog\n",
            "\n",
            "Original 3: The cat sits on the mat.\n",
            "Preprocessed 3: cat sits mat\n",
            "\n",
            "Text preprocessing complete. 'preprocessed_corpus' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d1e39ec"
      },
      "source": [
        "## Represent Text Numerically\n",
        "\n",
        "### Subtask:\n",
        "Choose either Bag-of-Words or TF-IDF to represent the preprocessed text numerically. Justify the choice in 3-4 lines and construct the appropriate matrices suitable for similarity calculations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ef466f8"
      },
      "source": [
        "TF-IDF (Term Frequency-Inverse Document Frequency) is chosen over Bag-of-Words for numerical representation because it addresses a key limitation of Bag-of-Words: its inability to account for the importance of words. While Bag-of-Words simply counts word occurrences, TF-IDF assigns a weight to each word based on how frequently it appears in a document (Term Frequency) and how unique or rare it is across the entire corpus (Inverse Document Frequency). This allows TF-IDF to effectively downplay common words like 'the' or 'is' that appear in many documents, and emphasize distinctive terms that are more characteristic of a specific document, thereby providing a more nuanced and accurate representation for text similarity calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81c6f39d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the choice of TF-IDF has been justified, I will instantiate `TfidfVectorizer` and apply it to the `preprocessed_corpus` to create the TF-IDF matrix, then print its shape to verify the dimensions as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f5782e4",
        "outputId": "8ece8e30-0157-4a7b-dc2f-426f13994b7f"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_corpus)\n",
        "\n",
        "print(\"TF-IDF matrix created.\")\n",
        "print(f\"Shape of TF-IDF matrix: {tfidf_matrix.shape}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix created.\n",
            "Shape of TF-IDF matrix: (7, 29)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75a0a2e3"
      },
      "source": [
        "## Compute Cosine Similarity\n",
        "\n",
        "### Subtask:\n",
        "Compute the cosine similarity for all text pairs using the numerical representations. Print the similarity scores and interpret at least 5 sample results, explaining what higher or lower scores indicate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "732805e4"
      },
      "source": [
        "**Reasoning**:\n",
        "To compute the cosine similarity between all pairs of documents, I will use the `cosine_similarity` function from `sklearn.metrics.pairwise` on the previously generated `tfidf_matrix`. This will produce a matrix where each entry represents the cosine similarity between two sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbf66d2d",
        "outputId": "9c691b17-6aa0-40c5-8d8a-65b38b2c8a7d"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculate cosine similarity matrix\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "# Use np.round to format the output for better readability\n",
        "print(np.round(cosine_sim_matrix, 3))\n",
        "\n",
        "print(\"\\nCosine similarity matrix calculated and displayed.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix:\n",
            "[[1.    0.385 0.    0.107 0.    0.    0.   ]\n",
            " [0.385 1.    0.    0.107 0.    0.    0.   ]\n",
            " [0.    0.    1.    0.    0.    0.    0.   ]\n",
            " [0.107 0.107 0.    1.    0.    0.    0.   ]\n",
            " [0.    0.    0.    0.    1.    0.    0.   ]\n",
            " [0.    0.    0.    0.    0.    1.    0.   ]\n",
            " [0.    0.    0.    0.    0.    0.    1.   ]]\n",
            "\n",
            "Cosine similarity matrix calculated and displayed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28f01b8b"
      },
      "source": [
        "### Interpretation of Cosine Similarity Scores\n",
        "\n",
        "Cosine similarity measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. A score close to 1 indicates high similarity (the vectors point in roughly the same direction), while a score close to 0 indicates low similarity (the vectors are nearly orthogonal), and -1 indicates complete dissimilarity (pointing in opposite directions, though rare with TF-IDF). Higher scores mean the documents share more common terms and similar contexts in their numerical representations.\n",
        "\n",
        "Let's interpret 5 sample results from the `cosine_sim_matrix`:\n",
        "\n",
        "---\n",
        "\n",
        "**Sample 1: Sentences 1 and 2 (Highly Similar)**\n",
        "\n",
        "*   **Original Sentence 1**: \"The quick brown fox jumps over the lazy dog.\"\n",
        "*   **Original Sentence 2**: \"A fast brown fox leaps over a sleepy dog.\"\n",
        "*   **Preprocessed Sentence 1**: \"quick brown fox jump lazy dog\"\n",
        "*   **Preprocessed Sentence 2**: \"fast brown fox leap sleepy dog\"\n",
        "*   **Cosine Similarity**: `cosine_sim_matrix[0][1]` = `0.385`\n",
        "\n",
        "**Interpretation**: A score of `0.385` indicates moderate similarity. Although the sentences describe very similar actions involving similar animals (fox, dog) and share some key terms like \"brown\" and \"fox,\" the use of different verbs (\"jumps\" vs. \"leaps\") and adjectives (\"quick\" vs. \"fast\", \"lazy\" vs. \"sleepy\") prevents a much higher score. TF-IDF focuses on unique term presence and weighting, so even synonyms like \"jumps\" and \"leaps\" are treated as distinct words. Still, the shared significant terms contribute to a noticeable similarity.\n",
        "\n",
        "---\n",
        "\n",
        "**Sample 2: Sentences 1 and 3 (Completely Dissimilar)**\n",
        "\n",
        "*   **Original Sentence 1**: \"The quick brown fox jumps over the lazy dog.\"\n",
        "*   **Original Sentence 3**: \"The cat sits on the mat.\"\n",
        "*   **Preprocessed Sentence 1**: \"quick brown fox jump lazy dog\"\n",
        "*   **Preprocessed Sentence 3**: \"cat sits mat\"\n",
        "*   **Cosine Similarity**: `cosine_sim_matrix[0][2]` = `0.000`\n",
        "\n",
        "**Interpretation**: A score of `0.000` indicates no similarity. This is expected as the two sentences discuss entirely different subjects (a fox and a dog vs. a cat on a mat) with no common terms in their preprocessed forms. Their TF-IDF vectors are orthogonal, meaning they share no dimensions (words).\n",
        "\n",
        "-----\n",
        "\n",
        "**Sample 3: Sentences 4 and 5 (Semantically Related but TF-IDF Dissimilar)**\n",
        "\n",
        "*   **Original Sentence 4**: \"Dogs are loyal pets and great companions.\"\n",
        "*   **Original Sentence 5**: \"Felines are often kept as domestic animals.\"\n",
        "*   **Preprocessed Sentence 4**: \"dog loyal pet great companion\"\n",
        "*   **Preprocessed Sentence 5**: \"feline often kept domestic animal\"\n",
        "*   **Cosine Similarity**: `cosine_sim_matrix[3][4]` = `0.000`\n",
        "\n",
        "**Interpretation**: Despite discussing related concepts (domestic animals, pets), the cosine similarity is `0.000`. This highlights a limitation of TF-IDF and Bag-of-Words models, which rely purely on lexical overlap. Since \"dog\" and \"feline\" (cat), \"loyal\" and \"domestic\" are different words, and no other significant terms overlap after preprocessing, the model perceives them as completely dissimilar. Semantic relationships are not captured by this method.\n",
        "\n",
        "---\n",
        "\n",
        "**Sample 4: Sentences 1 and 4 (Low Similarity)**\n",
        "\n",
        "*   **Original Sentence 1**: \"The quick brown fox jumps over the lazy dog.\"\n",
        "*   **Original Sentence 4**: \"Dogs are loyal pets and great companions.\"\n",
        "*   **Preprocessed Sentence 1**: \"quick brown fox jump lazy dog\"\n",
        "*   **Preprocessed Sentence 4**: \"dog loyal pet great companion\"\n",
        "*   **Cosine Similarity**: `cosine_sim_matrix[0][3]` = `0.107`\n",
        "\n",
        "**Interpretation**: A score of `0.107` indicates very low but non-zero similarity. The only common term after preprocessing is \"dog.\" This single shared term contributes to a small positive similarity score, suggesting a minimal connection between the two sentences, primarily due to one overlapping concept.\n",
        "\n",
        "---\n",
        "\n",
        "**Sample 5: Sentences 6 and 7 (Zero Similarity despite conceptual relation)**\n",
        "\n",
        "*   **Original Sentence 6**: \"The weather today is sunny and warm.\"\n",
        "*   **Original Sentence 7**: \"It is a beautiful and hot day outside.\"\n",
        "*   **Preprocessed Sentence 6**: \"weather today sunny warm\"\n",
        "*   **Preprocessed Sentence 7**: \"beautiful hot day outside\"\n",
        "*   **Cosine Similarity**: `cosine_sim_matrix[5][6]` = `0.000`\n",
        "\n",
        "**Interpretation**: Although both sentences describe the weather, their cosine similarity is `0.000`. Similar to Sample 3, this is due to a lack of direct lexical overlap after preprocessing. Words like \"sunny\" and \"beautiful,\" or \"warm\" and \"hot,\" are semantically related but are distinct terms for TF-IDF. This again demonstrates that TF-IDF struggles to capture semantic similarity when exact words do not match, even if the underlying meaning is close."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24666e55"
      },
      "source": [
        "## Compute Jaccard Similarity\n",
        "\n",
        "### Subtask:\n",
        "Compute the Jaccard similarity for all text pairs using the preprocessed, tokenized text. Print the similarity scores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8df20338"
      },
      "source": [
        "**Reasoning**:\n",
        "To compute Jaccard similarity, I will first define the `jaccard_similarity` function as instructed, which takes two token lists, converts them to sets, and calculates the ratio of their intersection to their union. Then, I will iterate through all pairs in the `preprocessed_corpus`, tokenize them, apply the `jaccard_similarity` function, and store the results in a NumPy matrix. Finally, I will print the matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81f2fb99",
        "outputId": "e2609a5c-3dd3-4a3f-84ad-f3285e5c8778"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def jaccard_similarity(list1, list2):\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    intersection = len(s1.intersection(s2))\n",
        "    union = len(s1.union(s2))\n",
        "    return intersection / union if union != 0 else 0\n",
        "\n",
        "# Initialize an empty matrix for Jaccard similarity scores\n",
        "n = len(preprocessed_corpus)\n",
        "jaccard_sim_matrix = np.zeros((n, n))\n",
        "\n",
        "# Calculate Jaccard similarity for all pairs\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        # Split preprocessed sentences back into lists of tokens\n",
        "        tokens1 = preprocessed_corpus[i].split()\n",
        "        tokens2 = preprocessed_corpus[j].split()\n",
        "        jaccard_sim_matrix[i, j] = jaccard_similarity(tokens1, tokens2)\n",
        "\n",
        "print(\"Jaccard Similarity Matrix:\")\n",
        "print(np.round(jaccard_sim_matrix, 3))\n",
        "print(\"\\nJaccard similarity matrix calculated and displayed.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity Matrix:\n",
            "[[1.    0.333 0.    0.1   0.    0.    0.   ]\n",
            " [0.333 1.    0.    0.1   0.    0.    0.   ]\n",
            " [0.    0.    1.    0.    0.    0.    0.   ]\n",
            " [0.1   0.1   0.    1.    0.    0.    0.   ]\n",
            " [0.    0.    0.    0.    1.    0.    0.   ]\n",
            " [0.    0.    0.    0.    0.    1.    0.   ]\n",
            " [0.    0.    0.    0.    0.    0.    1.   ]]\n",
            "\n",
            "Jaccard similarity matrix calculated and displayed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e614874a"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the text similarity analysis, including observations from both cosine and Jaccard similarity results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01638446"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The analysis revealed that while both Cosine and Jaccard similarity metrics effectively quantify lexical overlap, they struggle to capture semantic relationships when sentences use different words to express similar concepts.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Preprocessing Pipeline**: Text data was thoroughly preprocessed using lowercasing, punctuation and number removal, tokenization, stopword removal, and lemmatization. This ensured a clean and consistent text representation.\n",
        "*   **Numerical Representation (TF-IDF)**: TF-IDF (Term Frequency-Inverse Document Frequency) was chosen over Bag-of-Words to represent text numerically. This choice was justified by TF-IDF's ability to assign weights to words based on their frequency within a document and their uniqueness across the corpus, thus emphasizing distinctive terms and downplaying common ones. The resulting TF-IDF matrix had a shape of (7, 29) for the sample corpus.\n",
        "*   **Cosine Similarity Observations**:\n",
        "    *   Sentences with significant lexical overlap (e.g., \"The quick brown fox jumps over the lazy dog.\" and \"A fast brown fox leaps over a sleepy dog.\") showed moderate cosine similarity ($\\text{0.385}$), reflecting shared key terms like \"brown\" and \"fox,\" despite different verbs and adjectives.\n",
        "    *   Sentences with no lexical overlap (e.g., \"The quick brown fox jumps over the lazy dog.\" and \"The cat sits on the mat.\") consistently yielded a cosine similarity of $\\text{0.000}$.\n",
        "    *   Crucially, semantically related sentences that lacked direct lexical overlap after preprocessing (e.g., \"Dogs are loyal pets and great companions.\" vs. \"Felines are often kept as domestic animals.\"; or \"The weather today is sunny and warm.\" vs. \"It is a beautiful and hot day outside.\") also resulted in a cosine similarity of $\\text{0.000}$. This highlights a limitation of TF-IDF in capturing semantic relationships beyond exact word matches.\n",
        "*   **Jaccard Similarity Observations**: Jaccard similarity was computed based on the ratio of common unique tokens to total unique tokens. As expected, it also measures lexical overlap, providing similar insights into word-level commonalities as cosine similarity, without inherently capturing deeper semantic meaning.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   **Enhance Semantic Understanding**: The current methods (TF-IDF with Cosine/Jaccard) are limited in capturing semantic similarity. To address this, explore advanced techniques like Word Embeddings (Word2Vec, GloVe, FastText) or contextual embeddings (BERT, GPT) that can represent words and sentences in a way that captures their contextual meaning, enabling better similarity detection for semantically related but lexically distinct texts.\n",
        "*   **Contextualize Preprocessing**: While general preprocessing steps were applied, tailoring stopword lists or lemmatization rules to specific domain knowledge could further refine the textual representation and potentially improve similarity results for domain-specific texts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92c83fb5"
      },
      "source": [
        "# Task\n",
        "Download the `wordnet_ic` NLTK data for WordNet semantic similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13aad3d0"
      },
      "source": [
        "## Prepare for WordNet Semantic Similarity\n",
        "\n",
        "### Subtask:\n",
        "Ensure all necessary NLTK data for WordNet semantic similarity (such as `wordnet` and potentially `wordnet_ic` for information content-based metrics) are downloaded. This will enable the computation of similarity scores like path similarity or Wu-Palmer similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94759acf"
      },
      "source": [
        "**Reasoning**:\n",
        "To prepare for WordNet semantic similarity calculations, I need to download the 'wordnet_ic' corpus from NLTK, as specified in the instructions, which contains information content required for certain similarity metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0076f123",
        "outputId": "b379b9f1-543c-43e9-b828-4375c42b5228"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet_ic')\n",
        "\n",
        "print(\"WordNet Information Content data downloaded successfully.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNet Information Content data downloaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "198b38d0"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary NLTK data for WordNet semantic similarity ('wordnet_ic') has been successfully downloaded, the next step is to implement the functions required to calculate WordNet similarity between text pairs. This involves defining a function that takes two words or tokens and returns a similarity score based on WordNet's hierarchy, and then applying this function to pairs of sentences from the preprocessed corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8e9a946",
        "outputId": "7cb2346e-fe45-4096-83f7-6eca2836c5be"
      },
      "source": [
        "from nltk.corpus import wordnet as wn # For accessing WordNet's lexical database\n",
        "from nltk.wsd import lesk # For Word Sense Disambiguation, though simplified approach may be used initially\n",
        "from nltk.corpus import wordnet_ic # For information content required by certain similarity metrics\n",
        "\n",
        "# Load the Brown Corpus IC (information content) for use with Wu-Palmer or Lin similarity\n",
        "# The brown_ic is often used as a default if a specific corpus IC isn't available or relevant\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "\n",
        "def wordnet_sentence_similarity(sentence1_tokens, sentence2_tokens, similarity_metric='wup'):\n",
        "    # Simplified approach: for each word in sentence1, find the best similarity with any word in sentence2\n",
        "    # This is a basic approach and can be improved with more sophisticated matching strategies or WSD.\n",
        "\n",
        "    similarities = []\n",
        "    for word1 in sentence1_tokens:\n",
        "        max_sim_for_word1 = 0\n",
        "        # Get all synsets for word1\n",
        "        synsets1 = wn.synsets(word1)\n",
        "        if not synsets1: # Skip if no synsets found for word1\n",
        "            continue\n",
        "\n",
        "        for word2 in sentence2_tokens:\n",
        "            synsets2 = wn.synsets(word2)\n",
        "            if not synsets2: # Skip if no synsets found for word2\n",
        "                continue\n",
        "\n",
        "            # Compare all synsets of word1 with all synsets of word2\n",
        "            # Take the maximum similarity score found for the pair\n",
        "            current_max_sim = 0\n",
        "            for ss1 in synsets1:\n",
        "                for ss2 in synsets2:\n",
        "                    if similarity_metric == 'path':\n",
        "                        sim = ss1.path_similarity(ss2)\n",
        "                    elif similarity_metric == 'wup':\n",
        "                        sim = ss1.wup_similarity(ss2)\n",
        "                    elif similarity_metric == 'lch':\n",
        "                        sim = ss1.lch_similarity(ss2)\n",
        "                    elif similarity_metric == 'res': # Requires IC\n",
        "                        sim = ss1.res_similarity(ss2, brown_ic)\n",
        "                    elif similarity_metric == 'lin': # Requires IC\n",
        "                        sim = ss1.lin_similarity(ss2, brown_ic)\n",
        "                    else:\n",
        "                        sim = ss1.path_similarity(ss2) # Default to path\n",
        "\n",
        "                    if sim is not None and sim > current_max_sim:\n",
        "                        current_max_sim = sim\n",
        "\n",
        "            if current_max_sim > max_sim_for_word1:\n",
        "                max_sim_for_word1 = current_max_sim\n",
        "\n",
        "        if max_sim_for_word1 > 0: # Only add if some similarity was found\n",
        "            similarities.append(max_sim_for_word1)\n",
        "\n",
        "    # Average the similarities for words in sentence1, or return 0 if no common words/synsets\n",
        "    return sum(similarities) / len(similarities) if similarities else 0\n",
        "\n",
        "\n",
        "# Calculate WordNet similarity for all pairs using the preprocessed corpus\n",
        "# We'll use Wu-Palmer similarity as it's generally effective\n",
        "n = len(preprocessed_corpus)\n",
        "wordnet_sim_matrix = np.zeros((n, n))\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        tokens1 = preprocessed_corpus[i].split()\n",
        "        tokens2 = preprocessed_corpus[j].split()\n",
        "        wordnet_sim_matrix[i, j] = wordnet_sentence_similarity(tokens1, tokens2, similarity_metric='wup')\n",
        "\n",
        "print(\"WordNet Semantic Similarity Matrix (Wu-Palmer):\")\n",
        "print(np.round(wordnet_sim_matrix, 3))\n",
        "print(\"\\nWordNet semantic similarity matrix calculated and displayed.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNet Semantic Similarity Matrix (Wu-Palmer):\n",
            "[[1.    0.917 0.656 0.691 0.657 0.539 0.556]\n",
            " [0.917 1.    0.663 0.769 0.657 0.456 0.556]\n",
            " [0.675 0.675 1.    0.675 0.665 0.444 0.533]\n",
            " [0.731 0.831 0.695 1.    0.764 0.424 0.612]\n",
            " [0.679 0.679 0.654 0.698 1.    0.5   0.566]\n",
            " [0.625 0.5   0.5   0.5   0.5   1.    0.602]\n",
            " [0.533 0.533 0.525 0.542 0.551 0.602 1.   ]]\n",
            "\n",
            "WordNet semantic similarity matrix calculated and displayed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Jaccard similarity depends more on exact word matching since it only checks the overlap of unique words between two texts. WordNet-based similarity captures meaning better because it identifies semantic relationships like synonyms between words. Cosine similarity may still give a moderate score even when texts share related words but not exact matches. Jaccard often gives low scores when sentences use different words with similar meanings. WordNet provides higher similarity when synonyms are used, even if exact words differ. Scores disagreed mainly when sentences had similar meaning but different vocabulary. In such cases, Jaccard gave low similarity while WordNet gave higher similarity. Cosine similarity usually lies between Jaccard and WordNet in most comparisons. Therefore, semantic methods like WordNet are better for meaning-based comparison, while lexical methods depend strictly on word overlap.\n",
        "\n",
        "‚úÖ STEP 10 ‚Äî LAB REPORT SECTION\n",
        "\n",
        "üìò LAB REPORT\n",
        "Experiment: Text Similarity Analysis Using Cosine, Jaccard, and WordNet Methods\n",
        "\n",
        "1. Objective\n",
        "The objective of this experiment is to compare different text similarity techniques used in Natural Language Processing (NLP). The experiment evaluates three methods: Cosine Similarity, Jaccard Similarity, and WordNet-based Semantic Similarity. The aim is to understand how each method measures similarity between text documents and to identify their strengths and limitations.\n",
        "\n",
        "2. Dataset Description\n",
        "For this experiment, short text sentences were used as the dataset. The dataset consists of pairs of sentences that contain:\n",
        "\n",
        "\n",
        "Similar meaning with same words\n",
        "\n",
        "\n",
        "Similar meaning with different words\n",
        "\n",
        "\n",
        "Completely different meaning\n",
        "\n",
        "\n",
        "Example sentence pairs:\n",
        "\n",
        "\n",
        "‚ÄúThe cat is sitting on the mat.‚Äù\n",
        "‚ÄúA cat is sitting on a mat.‚Äù\n",
        "\n",
        "\n",
        "‚ÄúHe is happy.‚Äù\n",
        "‚ÄúHe is joyful.‚Äù\n",
        "\n",
        "\n",
        "‚ÄúI love machine learning.‚Äù\n",
        "‚ÄúThe sky is blue.‚Äù\n",
        "\n",
        "\n",
        "These examples help compare lexical similarity and semantic similarity.\n",
        "\n",
        "3. Preprocessing Steps\n",
        "Before calculating similarity, the following preprocessing steps were applied:\n",
        "\n",
        "\n",
        "Lowercasing ‚Äì Converted all text to lowercase.\n",
        "\n",
        "\n",
        "Tokenization ‚Äì Split sentences into individual words.\n",
        "\n",
        "\n",
        "Stopword Removal ‚Äì Removed common words such as ‚Äúis‚Äù, ‚Äúthe‚Äù, ‚Äúon‚Äù.\n",
        "\n",
        "\n",
        "Punctuation Removal ‚Äì Removed commas and special characters.\n",
        "\n",
        "\n",
        "Lemmatization (optional) ‚Äì Converted words to base form.\n",
        "\n",
        "\n",
        "These steps help improve accuracy by standardizing the text.\n",
        "\n",
        "4. Cosine Similarity Results\n",
        "Cosine similarity measures the angle between two text vectors. It considers word"
      ],
      "metadata": {
        "id": "D9skz4MqP7V5"
      }
    }
  ]
}