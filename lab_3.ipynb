{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNInb4W12H507aBfvjsT7SI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaya-02ly/akshaya2260-nlp/blob/main/lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPT:\n",
        "Parts of Speech (POS) tagging is the process of assigning a grammatical category (such as noun, verb, adjective, etc.) to each word\n",
        "(token) in a sentence based on its definition and context.\n"
      ],
      "metadata": {
        "id": "Txan2w_v-n29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKmEv9Xt6Ooe",
        "outputId": "23aa78b6-e006-48a2-ddca-ab09b137eac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Students', 'NNS'), ('are', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError\n",
        "sentence = \"Students are learning Natural Language Processing\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Students are learning Natural Language Processing\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3oWDciY8Cxf",
        "outputId": "97884044-3e63-47dd-8433-7e12afb26982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Students NOUN\n",
            "are AUX\n",
            "learning VERB\n",
            "Natural PROPN\n",
            "Language PROPN\n",
            "Processing NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying a startup in India.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YVK1Log8QjF",
        "outputId": "30410d09-a8a1-4d9d-b759-c5eeda0a9516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple PROPN NNP\n",
            "is AUX VBZ\n",
            "looking VERB VBG\n",
            "at ADP IN\n",
            "buying VERB VBG\n",
            "a DET DT\n",
            "startup NOUN NN\n",
            "in ADP IN\n",
            "India PROPN NNP\n",
            ". PUNCT .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Loving the new AI features üòç #AI #MachineLearning\"\n",
        "doc = nlp(text)\n",
        "nouns = []\n",
        "verbs = []\n",
        "for token in doc:\n",
        "    if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "        nouns.append(token.text)\n",
        "    elif token.pos_ == \"VERB\":\n",
        "        verbs.append(token.text)\n",
        "noun_freq = Counter(nouns)\n",
        "verb_freq = Counter(verbs)\n",
        "\n",
        "print(\"Noun Frequency:\", noun_freq)\n",
        "print(\"Verb Frequency:\", verb_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJgkq7Yb8bF1",
        "outputId": "a0544c99-b41d-407f-daff-d0230fec14da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Frequency: Counter({'AI': 2, 'üòç': 1, 'MachineLearning': 1})\n",
            "Verb Frequency: Counter({'Loving': 1, 'features': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS(SR UNIVERSITY)"
      ],
      "metadata": {
        "id": "LQvo1JgcANQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SRUniversity=\"\"\"The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India.\n",
        "It is in 150 acres, with both separate hostel facilities for boys and girls.\n",
        "There is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.\"\"\"\n"
      ],
      "metadata": {
        "id": "RqaaJLGSAQ7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError\n",
        "sentence = SRUniversity\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA8RE2V8A6PS",
        "outputId": "dc70ff2b-e3fa-49f1-f9ae-90c390b810fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('SR', 'NNP'), ('University', 'NNP'), ('campus', 'NN'), ('is', 'VBZ'), ('located', 'VBN'), ('in', 'IN'), ('Ananthasagar', 'NNP'), ('village', 'NN'), ('of', 'IN'), ('Hasanparthy', 'NNP'), ('Mandal', 'NNP'), ('in', 'IN'), ('Warangal', 'NNP'), (',', ','), ('Telangana', 'NNP'), (',', ','), ('India', 'NNP'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('in', 'IN'), ('150', 'CD'), ('acres', 'NNS'), (',', ','), ('with', 'IN'), ('both', 'DT'), ('separate', 'JJ'), ('hostel', 'NN'), ('facilities', 'NNS'), ('for', 'IN'), ('boys', 'NNS'), ('and', 'CC'), ('girls', 'NNS'), ('.', '.'), ('There', 'EX'), ('is', 'VBZ'), ('a', 'DT'), ('huge', 'JJ'), ('central', 'JJ'), ('library', 'NN'), ('along', 'IN'), ('with', 'IN'), ('Indias', 'NNP'), ('largest', 'JJS'), ('Technology', 'NN'), ('Business', 'NNP'), ('Incubator', 'NNP'), ('(', '('), ('TBI', 'NNP'), (')', ')'), ('in', 'IN'), ('tier', '$'), ('2', 'CD'), ('cities', 'NNS'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"\"\"The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India.\n",
        "It is in 150 acres, with both separate hostel facilities for boys and girls.\n",
        "There is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.\"\"\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrxl_7pfBL-s",
        "outputId": "fc56df8a-8f4e-4818-9f36-a54a8f6e893f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DET\n",
            "SR PROPN\n",
            "University PROPN\n",
            "campus NOUN\n",
            "is AUX\n",
            "located VERB\n",
            "in ADP\n",
            "Ananthasagar PROPN\n",
            "village NOUN\n",
            "of ADP\n",
            "Hasanparthy PROPN\n",
            "Mandal PROPN\n",
            "in ADP\n",
            "Warangal PROPN\n",
            ", PUNCT\n",
            "Telangana PROPN\n",
            ", PUNCT\n",
            "India PROPN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "It PRON\n",
            "is AUX\n",
            "in ADP\n",
            "150 NUM\n",
            "acres NOUN\n",
            ", PUNCT\n",
            "with ADP\n",
            "both DET\n",
            "separate ADJ\n",
            "hostel NOUN\n",
            "facilities NOUN\n",
            "for ADP\n",
            "boys NOUN\n",
            "and CCONJ\n",
            "girls NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "There PRON\n",
            "is VERB\n",
            "a DET\n",
            "huge ADJ\n",
            "central ADJ\n",
            "library NOUN\n",
            "along ADP\n",
            "with ADP\n",
            "Indias PROPN\n",
            "largest ADJ\n",
            "Technology PROPN\n",
            "Business PROPN\n",
            "Incubator PROPN\n",
            "( PUNCT\n",
            "TBI PROPN\n",
            ") PUNCT\n",
            "in ADP\n",
            "tier NOUN\n",
            "2 NUM\n",
            "cities NOUN\n",
            ". PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = (\"\"\"The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India.\n",
        "It is in 150 acres, with both separate hostel facilities for boys and girls.\n",
        "There is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.\"\"\")\n",
        "doc = nlp(text)\n",
        "nouns = []\n",
        "verbs = []\n",
        "for token in doc:\n",
        "    if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "        nouns.append(token.text)\n",
        "    elif token.pos_ == \"VERB\":\n",
        "        verbs.append(token.text)\n",
        "noun_freq = Counter(nouns)\n",
        "verb_freq = Counter(verbs)\n",
        "\n",
        "print(\"Noun Frequency:\", noun_freq)\n",
        "print(\"Verb Frequency:\", verb_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43MooxlvClkW",
        "outputId": "7dae3e65-eeb5-48a4-92f0-f349f405244c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Frequency: Counter({'SR': 1, 'University': 1, 'campus': 1, 'Ananthasagar': 1, 'village': 1, 'Hasanparthy': 1, 'Mandal': 1, 'Warangal': 1, 'Telangana': 1, 'India': 1, 'acres': 1, 'hostel': 1, 'facilities': 1, 'boys': 1, 'girls': 1, 'library': 1, 'Indias': 1, 'Technology': 1, 'Business': 1, 'Incubator': 1, 'TBI': 1, 'tier': 1, 'cities': 1})\n",
            "Verb Frequency: Counter({'located': 1, 'is': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "63mZeZOTD__B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "\n",
        "extraction_path = '/content/extracted_data'\n",
        "file_path = os.path.join(extraction_path, 'enriched_posts.json')\n",
        "\n",
        "# Initialize an empty list to store parsed JSON objects\n",
        "data = []\n",
        "\n",
        "# Read the file line by line and parse each JSON object\n",
        "with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            data.append(json.loads(line.strip()))\n",
        "        except json.JSONDecodeError as e:\n",
        "            # Optionally print problematic lines or log the error\n",
        "            # print(f\"Skipping malformed JSON line: {line.strip()} - Error: {e}\")\n",
        "            continue\n",
        "\n",
        "# Create a DataFrame from the list of JSON objects\n",
        "d = pd.DataFrame(data)\n",
        "print(d.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOiG21B7GWhx",
        "outputId": "62b23270-fe76-4454-c9a8-bda4c21cc777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 0\n",
            "0  Personal Growth\n",
            "1       Job Search\n",
            "2       Job Search\n",
            "3         LinkedIn\n",
            "4       Job Search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "extraction_path = '/content/extracted_data'\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "\n",
        "file_path = os.path.join(extraction_path, 'raw_posts.json')\n",
        "\n",
        "with zipfile.ZipFile(\"/content/archive.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extract(\"raw_posts.json\", extraction_path)\n",
        "\n",
        "# Read the entire file content as a single string\n",
        "with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    file_content = f.read()\n",
        "\n",
        "# Parse the entire content as a single JSON object/array\n",
        "try:\n",
        "    data = json.loads(file_content)\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error decoding JSON from {file_path}: {e}\")\n",
        "    data = [] # Fallback to empty list if decoding fails\n",
        "\n",
        "# Create a DataFrame from the list of JSON objects\n",
        "d = pd.DataFrame(data)\n",
        "print(d.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXztAHORIKVR",
        "outputId": "947b7542-60c0-43e1-f2e2-06049c13b142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  engagement\n",
            "0  Just saw a LinkedIn Influencer with 'Organic G...          90\n",
            "1  Jobseekers, this one‚Äôs for you.\\n Every applic...         347\n",
            "2  Looking for jobs on LinkedIn is like online da...         109\n",
            "3  LinkedIn scams be like: 'Congratulations, you'...         115\n",
            "4  sapne dekhna achi baat hai,\\nlekin job ka sapn...         545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError\n",
        "sentence = \"Students are learning Natural Language Processing\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "id": "upkiKaVeKC29"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}